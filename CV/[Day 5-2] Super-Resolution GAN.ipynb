{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Day 5-2] Super-Resolution GAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"73331ac466c244c190f7dd6381827a6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c029e4332ce948d7bb47b7030755c30d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_538910751bd94d25b83c480acde2b001","IPY_MODEL_4a9113ed32d245d283005f32c1b627d9","IPY_MODEL_658bc51374ea4e91a73d798d8258bcb3"]}},"c029e4332ce948d7bb47b7030755c30d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"538910751bd94d25b83c480acde2b001":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f41f7c742a714ebeb55b0c1b3b32f3ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_52c279d319334690937dccf8d54435a1"}},"4a9113ed32d245d283005f32c1b627d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b2f71b73a4ae404b829da1b6a90448e0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c5ea1512a4644963ac03dbeb984bb491"}},"658bc51374ea4e91a73d798d8258bcb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cfdf61dab6b14f71a8361dd84d9298fe","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:07&lt;00:00, 88.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae4844a8ed55482483da713594d53ccd"}},"f41f7c742a714ebeb55b0c1b3b32f3ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"52c279d319334690937dccf8d54435a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b2f71b73a4ae404b829da1b6a90448e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c5ea1512a4644963ac03dbeb984bb491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cfdf61dab6b14f71a8361dd84d9298fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ae4844a8ed55482483da713594d53ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"VzcJrC-7lXKJ"},"source":["# SRGAN: Super-Resolution Meets GAN\n","\n","Today, we will go through some basics about image super-resolution problem and GANs. On top of this, we will take a look at SRGAN (CVPR 2017), which incorporates super-resolution with GAN.\n","\n","![IMG](\n","https://drive.google.com/uc?export=view&id=1jTnfsavfzSL43evIrchYaSCnNx7CMKA3)\n","\n","* [CVPR 2017] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial\n","Network [(pdf)](https://arxiv.org/pdf/1609.04802.pdf)\n","* Video example\n","https://www.youtube.com/watch?v=sUhbIdSd6dc\n","\n","Credit: [Sagar Vinodabadu](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution), colab-friendly modification with theoretical explanation by [HS-YN](https://github.com/hs-yn)"]},{"cell_type":"markdown","metadata":{"id":"Enymh-WYVYf-"},"source":["## What is Image Super-Resolution?\n","\n","* Casually, for a given low-res image, single-image super-resolution (SISR) aims at reconstructing high-res image.\n","* Formally, our goal is to obtain transformation matrix that converts an arbitrary high-resolution image downsampled by a factor of k.\n","![IMG](https://drive.google.com/uc?export=view&id=1613aWXW06NSkeoPZPbdS9cUKFnvZh51Q)\n","* There are a few popular benchmarks, which include: Set5, Set14, SR291, DIV2K (bicubic-oriented), City100 (realistic)\n","* There are two types of evaluation metrics:\n","  * Objective metrics: Comparing SR with HR (GT), related to MSE\n","    * Peak Signal-to-Noise Ratio (PSNR)\n","    * Structural Similarity (SSIM)\n","  * Subjective metrics: *Which one is better?*\n","    * Mean Opinion Scores\n","* Solution\n","  * Traditionally, we formulated this problem as cost minimization problem (which involves rigorous optimization)\n","  * SRCNN (TPAMI 2015) first proposed the usage of CNN for SISR\n","  * VDSR (CVPR 2016) enabled deep NN via utilizing residual connection\n","  * Most of the works since then are based on deep learning\n","  * ...including **SRGAN**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"smW7tAfiZCLy"},"source":["## What is GAN?\n","\n","![IMG](https://miro.medium.com/max/1800/1*TKr1dtcNgJCA8uYY1OhmSg.png)\n","\n","* **Generator** generates realistic samples to deceive Discriminator\n","* **Discriminator** discerns real samples from synthetic samples\n","\n","![IMG](https://t1.daumcdn.net/cfile/tistory/993D64395C8E306F22)"]},{"cell_type":"markdown","metadata":{"id":"z-TPIf4lbgUE"},"source":["## SRGAN\n","\n","### Intuition\n","\n","![IMG](https://drive.google.com/uc?export=view&id=1bLJBnU74eMsrWDpL0_awT9ptjUdGXE2P)\n","\n","**MSE is not realistic**\n","\n","### Architecture\n","\n","![IMG](https://drive.google.com/uc?export=view&id=1v5Bv9h_iSlzjdrx13Bmj5OybnTB33xzN)\n","\n","* **Generator (SRResNet)**\n","* **Discriminator**\n","* Pretrained VGG19\n","\n","### Objective\n","\n","**Min-max**\n","\n","![IMG](https://drive.google.com/uc?export=view&id=1erKTPzw4bMnzefDp4KuTs-DXmID5Am2D)\n","\n","**Content Loss**\n","\n","![IMG](https://drive.google.com/uc?export=view&id=13j9vKw76c2YfceGotylPcYQZtxMdHOun)\n","\n","**Adversarial Loss**\n","\n","![IMG](https://drive.google.com/uc?export=view&id=1Qmj-EApCdjvcJ8FMUrcWhSWZAoPVYrqx)\n"]},{"cell_type":"markdown","metadata":{"id":"N-T0ULgtd4vc"},"source":["# Hands-on SRGAN\n","\n","Instead of ImageNet as in original paper, we will utilize COCO dataset, which is easier to obtain.\n","\n","For test split, we will use Set14, one of the representative benchmarks in SISR domain. [Download](https://drive.google.com/file/d/1vsw07sV8wGrRQ8UARe2fO5jjgy9QJy_E/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"6xlAoZECl43P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634627813237,"user_tz":-540,"elapsed":911092,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"9da17c7b-0ac3-40d4-d645-17a3949f58d6"},"source":["      # Train\n","!wget http://images.cocodataset.org/zips/train2014.zip\n","!unzip train2014.zip > /dev/null\n","!rm -f train2014.zip && echo \"Train split download complete!\"\n","!wget http://images.cocodataset.org/zips/val2014.zip\n","!unzip val2014.zip > /dev/null\n","!rm -f val2014.zip && echo \"Validation split download complete!\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-19 07:01:42--  http://images.cocodataset.org/zips/train2014.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.107.28\n","Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.107.28|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13510573713 (13G) [application/zip]\n","Saving to: ‘train2014.zip’\n","\n","train2014.zip       100%[===================>]  12.58G  93.0MB/s    in 4m 32s  \n","\n","2021-10-19 07:06:14 (47.3 MB/s) - ‘train2014.zip’ saved [13510573713/13510573713]\n","\n","Train split download complete!\n","--2021-10-19 07:12:01--  http://images.cocodataset.org/zips/val2014.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.86.44\n","Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.86.44|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6645013297 (6.2G) [application/zip]\n","Saving to: ‘val2014.zip’\n","\n","val2014.zip         100%[===================>]   6.19G  46.1MB/s    in 2m 1s   \n","\n","2021-10-19 07:14:02 (52.2 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n","\n","Validation split download complete!\n"]}]},{"cell_type":"code","metadata":{"id":"ZvBkc9fbr6bz"},"source":["# Test\n","# Directly download from https://drive.google.com/file/d/1vsw07sV8wGrRQ8UARe2fO5jjgy9QJy_E/view?usp=sharing\n","# Also, upload this file to colab: https://drive.google.com/file/d/1iQOZcF2VX6u5zH1drM1Go8L7nsQHCfN0/view?usp=sharing\n","!unzip Set14.zip > /dev/null \n","!rm -f Set14.zip && rm -f Set14.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojwCP3LInXK8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634628295047,"user_tz":-540,"elapsed":31796,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"31205da2-de77-47b8-f868-fad0be3b97aa"},"source":["!pip install tqdm\n","import os\n","import time\n","import json\n","import math\n","import random\n","\n","from tqdm import tqdm\n","from PIL import Image, ImageDraw, ImageFont\n","from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.utils.data import Dataset\n","import torch.backends.cudnn as cudnn\n","import torchvision.transforms.functional as FT"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"MFjdglkcqsya"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"H3e0qadppLyM"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cudnn.benchmark = True\n","\n","# Some constants\n","rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n","imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n","imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n","imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n","imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7aBDQxmptu1"},"source":["def convert_image(img, source, target):\n","    \"\"\"\n","    Convert an image from a source format to a target format.\n","    :param img: image\n","    :param source: source format, one of 'pil' (PIL image), '[0, 1]' or '[-1, 1]' (pixel value ranges)\n","    :param target: target format, one of 'pil' (PIL image), '[0, 255]', '[0, 1]', '[-1, 1]' (pixel value ranges),\n","                   'imagenet-norm' (pixel values standardized by imagenet mean and std.),\n","                   'y-channel' (luminance channel Y in the YCbCr color format, used to calculate PSNR and SSIM)\n","    :return: converted image\n","    \"\"\"\n","    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n","    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm',\n","                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n","\n","    # Convert from source to [0, 1]\n","    if source == 'pil':\n","        img = FT.to_tensor(img)\n","\n","    elif source == '[0, 1]':\n","        pass  # already in [0, 1]\n","\n","    elif source == '[-1, 1]':\n","        img = (img + 1.) / 2.\n","\n","    # Convert from [0, 1] to target\n","    if target == 'pil':\n","        img = FT.to_pil_image(img)\n","\n","    elif target == '[0, 255]':\n","        img = 255. * img\n","\n","    elif target == '[0, 1]':\n","        pass  # already in [0, 1]\n","\n","    elif target == '[-1, 1]':\n","        img = 2. * img - 1.\n","\n","    elif target == 'imagenet-norm':\n","        if img.ndimension() == 3:\n","            img = (img - imagenet_mean) / imagenet_std\n","        elif img.ndimension() == 4:\n","            img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n","\n","    elif target == 'y-channel':\n","        # Based on definitions at https://github.com/xinntao/BasicSR/wiki/Color-conversion-in-SR\n","        # torch.dot() does not work the same way as numpy.dot()\n","        # So, use torch.matmul() to find the dot product between the last dimension of an 4-D tensor and a 1-D tensor\n","        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n","\n","    return img\n","\n","\n","class ImageTransforms(object):\n","    \"\"\"\n","    Image transformation pipeline.\n","    \"\"\"\n","\n","    def __init__(self, split, crop_size, scaling_factor, lr_img_type, hr_img_type):\n","        \"\"\"\n","        :param split: one of 'train' or 'test'\n","        :param crop_size: crop size of HR images\n","        :param scaling_factor: LR images will be downsampled from the HR images by this factor\n","        :param lr_img_type: the target format for the LR image; see convert_image() above for available formats\n","        :param hr_img_type: the target format for the HR image; see convert_image() above for available formats\n","        \"\"\"\n","        self.split = split.lower()\n","        self.crop_size = crop_size\n","        self.scaling_factor = scaling_factor\n","        self.lr_img_type = lr_img_type\n","        self.hr_img_type = hr_img_type\n","\n","        assert self.split in {'train', 'test'}\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        :param img: a PIL source image from which the HR image will be cropped, and then downsampled to create the LR image\n","        :return: LR and HR images in the specified format\n","        \"\"\"\n","\n","        # Crop\n","        if self.split == 'train':\n","            # Take a random fixed-size crop of the image, which will serve as the high-resolution (HR) image\n","            left = random.randint(1, img.width - self.crop_size)\n","            top = random.randint(1, img.height - self.crop_size)\n","            right = left + self.crop_size\n","            bottom = top + self.crop_size\n","            hr_img = img.crop((left, top, right, bottom))\n","        else:\n","            # Take the largest possible center-crop of it such that its dimensions are perfectly divisible by the scaling factor\n","            x_remainder = img.width % self.scaling_factor\n","            y_remainder = img.height % self.scaling_factor\n","            left = x_remainder // 2\n","            top = y_remainder // 2\n","            right = left + (img.width - x_remainder)\n","            bottom = top + (img.height - y_remainder)\n","            hr_img = img.crop((left, top, right, bottom))\n","\n","        # Downsize this crop to obtain a low-resolution version of it\n","        lr_img = hr_img.resize((int(hr_img.width / self.scaling_factor), int(hr_img.height / self.scaling_factor)),\n","                               Image.BICUBIC)\n","\n","        # Sanity check\n","        assert hr_img.width == lr_img.width * self.scaling_factor and hr_img.height == lr_img.height * self.scaling_factor\n","\n","        # Convert the LR and HR image to the required type\n","        lr_img = convert_image(lr_img, source='pil', target=self.lr_img_type)\n","        hr_img = convert_image(hr_img, source='pil', target=self.hr_img_type)\n","\n","        return lr_img, hr_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqi94i5Pp8fM"},"source":["class AverageMeter(object):\n","    \"\"\"\n","    Keeps track of most recent, average, sum, and count of a metric.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def clip_gradient(optimizer, grad_clip):\n","    \"\"\"\n","    Clips gradients computed during backpropagation to avoid explosion of gradients.\n","    :param optimizer: optimizer with the gradients to be clipped\n","    :param grad_clip: clip value\n","    \"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group['params']:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)\n","\n","\n","def save_checkpoint(state, filename):\n","    \"\"\"\n","    Save model checkpoint.\n","    :param state: checkpoint contents\n","    \"\"\"\n","\n","    torch.save(state, filename)\n","\n","\n","def adjust_learning_rate(optimizer, shrink_factor):\n","    \"\"\"\n","    Shrinks learning rate by a specified factor.\n","    :param optimizer: optimizer whose learning rate must be shrunk.\n","    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n","    \"\"\"\n","\n","    print(\"\\nDECAYING learning rate.\")\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = param_group['lr'] * shrink_factor\n","    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UBGo-Nop-5a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634628837569,"user_tz":-540,"elapsed":240387,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"27fb8045-d1ed-47c9-8982-0b279af7228e"},"source":["def create_data_lists(train_folders, test_folders, min_size, output_folder):\n","    \"\"\"\n","    Create lists for images in the training set and each of the test sets.\n","    :param train_folders: folders containing the training images; these will be merged\n","    :param test_folders: folders containing the test images; each test folder will form its own test set\n","    :param min_size: minimum width and height of images to be considered\n","    :param output_folder: save data lists here\n","    \"\"\"\n","    print(\"\\nCreating data lists... this may take some time.\\n\")\n","    train_images = list()\n","    for d in train_folders:\n","        for i in tqdm(os.listdir(d), desc=d):\n","            img_path = os.path.join(d, i)\n","            img = Image.open(img_path, mode='r')\n","            if img.width >= min_size and img.height >= min_size:\n","                train_images.append(img_path)\n","    print(\"There are %d images in the training data.\\n\" % len(train_images))\n","    with open(os.path.join(output_folder, 'train_images.json'), 'w') as j:\n","        json.dump(train_images, j)\n","\n","    for d in tqdm(test_folders):\n","        test_images = list()\n","        test_name = d.split(\"/\")[-1]\n","        for i in tqdm(os.listdir(d), desc=d):\n","            img_path = os.path.join(d, i)\n","            img = Image.open(img_path, mode='r')\n","            if img.width >= min_size and img.height >= min_size:\n","                test_images.append(img_path)\n","        print(\"There are %d images in the %s test data.\\n\" % (len(test_images), test_name))\n","        with open(os.path.join(output_folder, test_name + '_test_images.json'), 'w') as j:\n","            json.dump(test_images, j)\n","\n","    print(\"JSONS containing lists of Train and Test images have been saved to %s\\n\" % output_folder)\n","\n","create_data_lists(train_folders=['./train2014', './val2014'],\n","                  test_folders=['./Set14/original'],\n","                  min_size=100,\n","                  output_folder='./')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creating data lists... this may take some time.\n","\n"]},{"output_type":"stream","name":"stderr","text":["./train2014: 100%|██████████| 82783/82783 [02:43<00:00, 506.59it/s]\n","./val2014: 100%|██████████| 40504/40504 [01:16<00:00, 530.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["There are 123285 images in the training data.\n","\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1 [00:00<?, ?it/s]\n","./Set14/original: 100%|██████████| 14/14 [00:00<00:00, 452.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["There are 14 images in the original test data.\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  9.26it/s]"]},{"output_type":"stream","name":"stdout","text":["JSONS containing lists of Train and Test images have been saved to ./\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z55QxLfGqeWS"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"RDIdEdsPqa_E"},"source":["class SRDataset(Dataset):\n","    \"\"\"\n","    A PyTorch Dataset to be used by a PyTorch DataLoader.\n","    \"\"\"\n","\n","    def __init__(self, data_folder, split, crop_size, scaling_factor, lr_img_type, hr_img_type, test_data_name=None):\n","        \"\"\"\n","        :param data_folder: # folder with JSON data files\n","        :param split: one of 'train' or 'test'\n","        :param crop_size: crop size of target HR images\n","        :param scaling_factor: the input LR images will be downsampled from the target HR images by this factor; the scaling done in the super-resolution\n","        :param lr_img_type: the format for the LR image supplied to the model; see convert_image() in utils.py for available formats\n","        :param hr_img_type: the format for the HR image supplied to the model; see convert_image() in utils.py for available formats\n","        :param test_data_name: if this is the 'test' split, which test dataset? (for example, \"Set14\")\n","        \"\"\"\n","\n","        self.data_folder = data_folder\n","        self.split = split.lower()\n","        self.crop_size = int(crop_size)\n","        self.scaling_factor = int(scaling_factor)\n","        self.lr_img_type = lr_img_type\n","        self.hr_img_type = hr_img_type\n","        self.test_data_name = test_data_name\n","\n","        assert self.split in {'train', 'test'}\n","        if self.split == 'test' and self.test_data_name is None:\n","            raise ValueError(\"Please provide the name of the test dataset!\")\n","        assert lr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm'}\n","        assert hr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm'}\n","\n","        # If this is a training dataset, then crop dimensions must be perfectly divisible by the scaling factor\n","        # (If this is a test dataset, images are not cropped to a fixed size, so this variable isn't used)\n","        if self.split == 'train':\n","            assert self.crop_size % self.scaling_factor == 0, \"Crop dimensions are not perfectly divisible by scaling factor! This will lead to a mismatch in the dimensions of the original HR patches and their super-resolved (SR) versions!\"\n","\n","        # Read list of image-paths\n","        if self.split == 'train':\n","            with open(os.path.join(data_folder, 'train_images.json'), 'r') as j:\n","                self.images = json.load(j)\n","        else:\n","            with open(os.path.join(data_folder, self.test_data_name + '_test_images.json'), 'r') as j:\n","                self.images = json.load(j)\n","\n","        # Select the correct set of transforms\n","        self.transform = ImageTransforms(split=self.split,\n","                                         crop_size=self.crop_size,\n","                                         scaling_factor=self.scaling_factor,\n","                                         lr_img_type=self.lr_img_type,\n","                                         hr_img_type=self.hr_img_type)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        This method is required to be defined for use in the PyTorch DataLoader.\n","        :param i: index to retrieve\n","        :return: the 'i'th pair LR and HR images to be fed into the model\n","        \"\"\"\n","        # Read image\n","        img = Image.open(self.images[i], mode='r')\n","        img = img.convert('RGB')\n","        if img.width <= 96 or img.height <= 96:\n","            print(self.images[i], img.width, img.height)\n","        lr_img, hr_img = self.transform(img)\n","\n","        return lr_img, hr_img\n","\n","    def __len__(self):\n","        \"\"\"\n","        This method is required to be defined for use in the PyTorch DataLoader.\n","        :return: size of this data (in number of images)\n","        \"\"\"\n","        return len(self.images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vdRkuW1HrKHR"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"7p3FKDfUq0_l"},"source":["class ConvolutionalBlock(nn.Module):\n","    \"\"\"\n","    A convolutional block, comprising convolutional, BN, activation layers.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n","        \"\"\"\n","        :param in_channels: number of input channels\n","        :param out_channels: number of output channe;s\n","        :param kernel_size: kernel size\n","        :param stride: stride\n","        :param batch_norm: include a BN layer?\n","        :param activation: Type of activation; None if none\n","        \"\"\"\n","        super(ConvolutionalBlock, self).__init__()\n","\n","        if activation is not None:\n","            activation = activation.lower()\n","            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n","\n","        # A container that will hold the layers in this convolutional block\n","        layers = list()\n","\n","        # A convolutional layer\n","        layers.append(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n","                      padding=kernel_size // 2))\n","\n","        # A batch normalization (BN) layer, if wanted\n","        if batch_norm is True:\n","            layers.append(nn.BatchNorm2d(num_features=out_channels))\n","\n","        # An activation layer, if wanted\n","        if activation == 'prelu':\n","            layers.append(nn.PReLU())\n","        elif activation == 'leakyrelu':\n","            layers.append(nn.LeakyReLU(0.2))\n","        elif activation == 'tanh':\n","            layers.append(nn.Tanh())\n","\n","        # Put together the convolutional block as a sequence of the layers in this container\n","        self.conv_block = nn.Sequential(*layers)\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Forward propagation.\n","        :param input: input images, a tensor of size (N, in_channels, w, h)\n","        :return: output images, a tensor of size (N, out_channels, w, h)\n","        \"\"\"\n","        output = self.conv_block(input)  # (N, out_channels, w, h)\n","\n","        return output\n","\n","\n","class SubPixelConvolutionalBlock(nn.Module):\n","    \"\"\"\n","    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n","    \"\"\"\n","\n","    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n","        \"\"\"\n","        :param kernel_size: kernel size of the convolution\n","        :param n_channels: number of input and output channels\n","        :param scaling_factor: factor to scale input images by (along both dimensions)\n","        \"\"\"\n","        super(SubPixelConvolutionalBlock, self).__init__()\n","\n","        # A convolutional layer that increases the number of channels by scaling factor^2, followed by pixel shuffle and PReLU\n","        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n","                              kernel_size=kernel_size, padding=kernel_size // 2)\n","        # These additional channels are shuffled to form additional pixels, upscaling each dimension by the scaling factor\n","        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n","        self.prelu = nn.PReLU()\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Forward propagation.\n","        :param input: input images, a tensor of size (N, n_channels, w, h)\n","        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n","        \"\"\"\n","        output = self.conv(input)  # (N, n_channels * scaling factor^2, w, h)\n","        output = self.pixel_shuffle(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n","        output = self.prelu(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n","\n","        return output\n","\n","\n","class ResidualBlock(nn.Module):\n","    \"\"\"\n","    A residual block, comprising two convolutional blocks with a residual connection across them.\n","    \"\"\"\n","\n","    def __init__(self, kernel_size=3, n_channels=64):\n","        \"\"\"\n","        :param kernel_size: kernel size\n","        :param n_channels: number of input and output channels (same because the input must be added to the output)\n","        \"\"\"\n","        super(ResidualBlock, self).__init__()\n","\n","        # The first convolutional block\n","        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                                              batch_norm=True, activation='PReLu')\n","\n","        # The second convolutional block\n","        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n","                                              batch_norm=True, activation=None)\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Forward propagation.\n","        :param input: input images, a tensor of size (N, n_channels, w, h)\n","        :return: output images, a tensor of size (N, n_channels, w, h)\n","        \"\"\"\n","        residual = input  # (N, n_channels, w, h)\n","        output = self.conv_block1(input)  # (N, n_channels, w, h)\n","        output = self.conv_block2(output)  # (N, n_channels, w, h)\n","        output = output + residual  # (N, n_channels, w, h)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZPWSPSdsZbD"},"source":["class SRResNet(nn.Module):\n","    \"\"\"\n","    The SRResNet, as defined in the paper.\n","    \"\"\"\n","\n","    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n","        \"\"\"\n","        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n","        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n","        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n","        :param n_blocks: number of residual blocks\n","        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n","        \"\"\"\n","        super(SRResNet, self).__init__()\n","\n","        # Scaling factor must be 2, 4, or 8\n","        scaling_factor = int(scaling_factor)\n","        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n","\n","        # The first convolutional block\n","        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n","                                              batch_norm=False, activation='PReLu')\n","\n","        # A sequence of n_blocks residual blocks, each containing a skip-connection across the block\n","        self.residual_blocks = nn.Sequential(\n","            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n","\n","        # Another convolutional block\n","        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n","                                              kernel_size=small_kernel_size,\n","                                              batch_norm=True, activation=None)\n","\n","        # Upscaling is done by sub-pixel convolution, with each such block upscaling by a factor of 2\n","        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n","        self.subpixel_convolutional_blocks = nn.Sequential(\n","            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n","              in range(n_subpixel_convolution_blocks)])\n","\n","        # The last convolutional block\n","        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n","                                              batch_norm=False, activation='Tanh')\n","\n","    def forward(self, lr_imgs):\n","        \"\"\"\n","        Forward prop.\n","        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n","        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n","        \"\"\"\n","        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n","        residual = output  # (N, n_channels, w, h)\n","        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n","        output = self.conv_block2(output)  # (N, n_channels, w, h)\n","        output = output + residual  # (N, n_channels, w, h)\n","        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n","        sr_imgs = self.conv_block3(output)  # (N, 3, w * scaling factor, h * scaling factor)\n","\n","        return sr_imgs\n","\n","\n","class Generator(nn.Module):\n","    \"\"\"\n","    The generator in the SRGAN, as defined in the paper. Architecture identical to the SRResNet.\n","    \"\"\"\n","\n","    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n","        \"\"\"\n","        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n","        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n","        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n","        :param n_blocks: number of residual blocks\n","        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n","        \"\"\"\n","        super(Generator, self).__init__()\n","\n","        # The generator is simply an SRResNet, as above\n","        self.net = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n","                            n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n","\n","    def initialize_with_srresnet(self, srresnet_checkpoint):\n","        \"\"\"\n","        Initialize with weights from a trained SRResNet.\n","        :param srresnet_checkpoint: checkpoint filepath\n","        \"\"\"\n","        srresnet = torch.load(srresnet_checkpoint, map_location=torch.device(device))['model']\n","        self.net.load_state_dict(srresnet.state_dict())\n","\n","        print(\"\\nLoaded weights from pre-trained SRResNet.\\n\")\n","\n","    def forward(self, lr_imgs):\n","        \"\"\"\n","        Forward prop.\n","        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n","        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n","        \"\"\"\n","        sr_imgs = self.net(lr_imgs)  # (N, n_channels, w * scaling factor, h * scaling factor)\n","\n","        return sr_imgs\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"\n","    The discriminator in the SRGAN, as defined in the paper.\n","    \"\"\"\n","\n","    def __init__(self, kernel_size=3, n_channels=64, n_blocks=8, fc_size=1024):\n","        \"\"\"\n","        :param kernel_size: kernel size in all convolutional blocks\n","        :param n_channels: number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n","        :param n_blocks: number of convolutional blocks\n","        :param fc_size: size of the first fully connected layer\n","        \"\"\"\n","        super(Discriminator, self).__init__()\n","\n","        in_channels = 3\n","\n","        # A series of convolutional blocks\n","        # The first, third, fifth (and so on) convolutional blocks increase the number of channels but retain image size\n","        # The second, fourth, sixth (and so on) convolutional blocks retain the same number of channels but halve image size\n","        # The first convolutional block is unique because it does not employ batch normalization\n","        conv_blocks = list()\n","        for i in range(n_blocks):\n","            out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n","            conv_blocks.append(\n","                ConvolutionalBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n","                                   stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0, activation='LeakyReLu'))\n","            in_channels = out_channels\n","        self.conv_blocks = nn.Sequential(*conv_blocks)\n","\n","        # An adaptive pool layer that resizes it to a standard size\n","        # For the default input size of 96 and 8 convolutional blocks, this will have no effect\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","        self.fc1 = nn.Linear(out_channels * 6 * 6, fc_size)\n","\n","        self.leaky_relu = nn.LeakyReLU(0.2)\n","\n","        self.fc2 = nn.Linear(1024, 1)\n","\n","        # Don't need a sigmoid layer because the sigmoid operation is performed by PyTorch's nn.BCEWithLogitsLoss()\n","\n","    def forward(self, imgs):\n","        \"\"\"\n","        Forward propagation.\n","        :param imgs: high-resolution or super-resolution images which must be classified as such, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n","        :return: a score (logit) for whether it is a high-resolution image, a tensor of size (N)\n","        \"\"\"\n","        batch_size = imgs.size(0)\n","        output = self.conv_blocks(imgs)\n","        output = self.adaptive_pool(output)\n","        output = self.fc1(output.view(batch_size, -1))\n","        output = self.leaky_relu(output)\n","        logit = self.fc2(output)\n","\n","        return logit\n","\n","\n","class TruncatedVGG19(nn.Module):\n","    \"\"\"\n","    A truncated VGG19 network, such that its output is the 'feature map obtained by the j-th convolution (after activation)\n","    before the i-th maxpooling layer within the VGG19 network', as defined in the paper.\n","    Used to calculate the MSE loss in this VGG feature-space, i.e. the VGG loss.\n","    \"\"\"\n","\n","    def __init__(self, i, j):\n","        \"\"\"\n","        :param i: the index i in the definition above\n","        :param j: the index j in the definition above\n","        \"\"\"\n","        super(TruncatedVGG19, self).__init__()\n","\n","        # Load the pre-trained VGG19 available in torchvision\n","        vgg19 = torchvision.models.vgg19(pretrained=True)\n","\n","        maxpool_counter = 0\n","        conv_counter = 0\n","        truncate_at = 0\n","        # Iterate through the convolutional section (\"features\") of the VGG19\n","        for layer in vgg19.features.children():\n","            truncate_at += 1\n","\n","            # Count the number of maxpool layers and the convolutional layers after each maxpool\n","            if isinstance(layer, nn.Conv2d):\n","                conv_counter += 1\n","            if isinstance(layer, nn.MaxPool2d):\n","                maxpool_counter += 1\n","                conv_counter = 0\n","\n","            # Break if we reach the jth convolution after the (i - 1)th maxpool\n","            if maxpool_counter == i - 1 and conv_counter == j:\n","                break\n","\n","        # Check if conditions were satisfied\n","        assert maxpool_counter == i - 1 and conv_counter == j, \"One or both of i=%d and j=%d are not valid choices for the VGG19!\" % (\n","            i, j)\n","\n","        # Truncate to the jth convolution (+ activation) before the ith maxpool layer\n","        self.truncated_vgg19 = nn.Sequential(*list(vgg19.features.children())[:truncate_at + 1])\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Forward propagation\n","        :param input: high-resolution or super-resolution images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n","        :return: the specified VGG19 feature map, a tensor of size (N, feature_map_channels, feature_map_w, feature_map_h)\n","        \"\"\"\n","        output = self.truncated_vgg19(input)  # (N, feature_map_channels, feature_map_w, feature_map_h)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XsC489bscFl"},"source":["## Training SRResNet"]},{"cell_type":"code","metadata":{"id":"5wIiDGSgsgOU"},"source":["# Data parameters\n","data_folder = './'  # folder with JSON data files\n","crop_size = 96  # crop size of target HR images\n","scaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n","\n","# Model parameters\n","large_kernel_size = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n","small_kernel_size = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n","n_channels = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n","n_blocks = 16  # number of residual blocks\n","\n","# Learning parameters\n","checkpoint = None  # path to model checkpoint, None if none\n","batch_size = 16  # batch size\n","start_epoch = 0  # start at this epoch\n","iterations = 1e6  # number of training iterations\n","workers = 4  # number of workers for loading data in the DataLoader\n","print_freq = 100  # print training status once every __ batches\n","lr = 1e-4  # learning rate\n","grad_clip = None  # clip if gradients are exploding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jtQEI7vNspXI","colab":{"base_uri":"https://localhost:8080/","height":739},"executionInfo":{"status":"error","timestamp":1634629711715,"user_tz":-540,"elapsed":250046,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"8d5773c6-fb5a-4993-fde3-30feb66c185a"},"source":["# Initialize model or load checkpoint\n","if checkpoint is None:\n","    model = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n","                     n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n","    # Initialize the optimizer\n","    optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n","                                 lr=lr)\n","\n","else:\n","    checkpoint = torch.load(checkpoint)\n","    start_epoch = checkpoint['epoch'] + 1\n","    model = checkpoint['model']\n","    optimizer = checkpoint['optimizer']\n","\n","# Move to default device\n","model = model.to(device)\n","criterion = nn.MSELoss().to(device)\n","\n","# Custom dataloaders\n","train_dataset = SRDataset(data_folder,\n","                          split='train',\n","                          crop_size=crop_size,\n","                          scaling_factor=scaling_factor,\n","                          lr_img_type='imagenet-norm',\n","                          hr_img_type='[-1, 1]')\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers,\n","                                           pin_memory=True)  # note that we're passing the collate function here\n","\n","# Total number of epochs to train for\n","epochs = int(iterations // len(train_loader) + 1)\n","\n","# Epochs\n","for epoch in range(start_epoch, epochs):\n","    \"\"\"\n","    One epoch's training.\n","    :param train_loader: DataLoader for training data\n","    :param model: model\n","    :param criterion: content loss function (Mean Squared-Error loss)\n","    :param optimizer: optimizer\n","    :param epoch: epoch number\n","    \"\"\"\n","    model.train()  # training mode enables batch normalization\n","\n","    batch_time = AverageMeter()  # forward prop. + back prop. time\n","    data_time = AverageMeter()  # data loading time\n","    losses = AverageMeter()  # loss\n","\n","    start = time.time()\n","\n","    # Batches\n","    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n","        data_time.update(time.time() - start)\n","\n","        # Move to default device\n","        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24), imagenet-normed\n","        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96), in [-1, 1]\n","\n","        # Forward prop.\n","        sr_imgs = model(lr_imgs)  # (N, 3, 96, 96), in [-1, 1]\n","\n","        # Loss\n","        loss = criterion(sr_imgs, hr_imgs)  # scalar\n","\n","        # Backward prop.\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # Clip gradients, if necessary\n","        if grad_clip is not None:\n","            clip_gradient(optimizer, grad_clip)\n","\n","        # Update model\n","        optimizer.step()\n","\n","        # Keep track of loss\n","        losses.update(loss.item(), lr_imgs.size(0))\n","\n","        # Keep track of batch time\n","        batch_time.update(time.time() - start)\n","\n","        # Reset start time\n","        start = time.time()\n","\n","        # Print status\n","        if i % print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]----'\n","                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})----'\n","                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})----'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader),\n","                                                                    batch_time=batch_time,\n","                                                                    data_time=data_time, loss=losses))\n","\n","    # Save checkpoint\n","    torch.save({'epoch': epoch,\n","                'model': model,\n","                'optimizer': optimizer},\n","               'checkpoint_srresnet.pth.tar')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [0][0/7706]----Batch Time 2.663 (2.663)----Data Time 0.748 (0.748)----Loss 0.3384 (0.3384)\n","Epoch: [0][100/7706]----Batch Time 0.177 (0.195)----Data Time 0.001 (0.009)----Loss 0.0407 (0.0743)\n","Epoch: [0][200/7706]----Batch Time 0.182 (0.182)----Data Time 0.001 (0.005)----Loss 0.0247 (0.0553)\n","Epoch: [0][300/7706]----Batch Time 0.166 (0.178)----Data Time 0.001 (0.003)----Loss 0.0224 (0.0475)\n","Epoch: [0][400/7706]----Batch Time 0.153 (0.176)----Data Time 0.000 (0.003)----Loss 0.0306 (0.0424)\n","Epoch: [0][500/7706]----Batch Time 0.164 (0.175)----Data Time 0.000 (0.002)----Loss 0.0222 (0.0391)\n","Epoch: [0][600/7706]----Batch Time 0.197 (0.174)----Data Time 0.000 (0.002)----Loss 0.0342 (0.0368)\n","Epoch: [0][700/7706]----Batch Time 0.156 (0.174)----Data Time 0.001 (0.002)----Loss 0.0259 (0.0350)\n","Epoch: [0][800/7706]----Batch Time 0.154 (0.173)----Data Time 0.000 (0.002)----Loss 0.0349 (0.0337)\n","Epoch: [0][900/7706]----Batch Time 0.188 (0.173)----Data Time 0.001 (0.002)----Loss 0.0223 (0.0325)\n","Epoch: [0][1000/7706]----Batch Time 0.188 (0.173)----Data Time 0.000 (0.002)----Loss 0.0228 (0.0315)\n","Epoch: [0][1100/7706]----Batch Time 0.163 (0.172)----Data Time 0.000 (0.002)----Loss 0.0290 (0.0307)\n","Epoch: [0][1200/7706]----Batch Time 0.199 (0.172)----Data Time 0.001 (0.002)----Loss 0.0256 (0.0299)\n","Epoch: [0][1300/7706]----Batch Time 0.161 (0.172)----Data Time 0.000 (0.001)----Loss 0.0295 (0.0294)\n","Epoch: [0][1400/7706]----Batch Time 0.151 (0.172)----Data Time 0.000 (0.001)----Loss 0.0195 (0.0288)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-1680b5474925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Forward prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msr_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_imgs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, 3, 96, 96), in [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2dce2c6a4ad6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, lr_imgs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_imgs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, 3, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-971d49181e06>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m  \u001b[0;31m# (N, n_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-971d49181e06>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, out_channels, w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         )\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2282\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"eMOnUFiiub6x"},"source":["## Training SRGAN\n","\n","Download SRResNet pretrained weight [here](https://drive.google.com/file/d/1-KDzq38-rMrFauxD565URAeraDEbz2MV/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"MudfasGMufgs"},"source":["# Data parameters\n","data_folder = './'  # folder with JSON data files\n","crop_size = 96  # crop size of target HR images\n","scaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n","\n","# Generator parameters\n","large_kernel_size_g = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n","small_kernel_size_g = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n","n_channels_g = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n","n_blocks_g = 16  # number of residual blocks\n","srresnet_checkpoint = \"./checkpoint_srresnet.pth.tar\"  # filepath of the trained SRResNet checkpoint used for initialization\n","\n","# Discriminator parameters\n","kernel_size_d = 3  # kernel size in all convolutional blocks\n","n_channels_d = 64  # number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n","n_blocks_d = 8  # number of convolutional blocks\n","fc_size_d = 1024  # size of the first fully connected layer\n","\n","# Learning parameters\n","checkpoint = None  # path to model (SRGAN) checkpoint, None if none\n","batch_size = 16  # batch size\n","start_epoch = 0  # start at this epoch\n","iterations = 2e5  # number of training iterations\n","workers = 4  # number of workers for loading data in the DataLoader\n","vgg19_i = 5  # the index i in the definition for VGG loss; see paper or models.py\n","vgg19_j = 4  # the index j in the definition for VGG loss; see paper or models.py\n","beta = 1e-3  # the coefficient to weight the adversarial loss in the perceptual loss\n","print_freq = 100  # print training status once every __ batches\n","lr = 1e-4  # learning rate\n","grad_clip = None  # clip if gradients are exploding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fj1FtlUluhw1","colab":{"base_uri":"https://localhost:8080/","height":993,"referenced_widgets":["73331ac466c244c190f7dd6381827a6b","c029e4332ce948d7bb47b7030755c30d","538910751bd94d25b83c480acde2b001","4a9113ed32d245d283005f32c1b627d9","658bc51374ea4e91a73d798d8258bcb3","f41f7c742a714ebeb55b0c1b3b32f3ae","52c279d319334690937dccf8d54435a1","b2f71b73a4ae404b829da1b6a90448e0","c5ea1512a4644963ac03dbeb984bb491","cfdf61dab6b14f71a8361dd84d9298fe","ae4844a8ed55482483da713594d53ccd"]},"executionInfo":{"status":"error","timestamp":1634630123317,"user_tz":-540,"elapsed":49991,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"7283ebe7-db9f-4361-ae6b-9e1491a82b1f"},"source":["# Initialize model or load checkpoint\n","if checkpoint is None:\n","    # Generator\n","    generator = Generator(large_kernel_size=large_kernel_size_g,\n","                          small_kernel_size=small_kernel_size_g,\n","                          n_channels=n_channels_g,\n","                          n_blocks=n_blocks_g,\n","                          scaling_factor=scaling_factor)\n","\n","    # Initialize generator network with pretrained SRResNet\n","    generator.initialize_with_srresnet(srresnet_checkpoint=srresnet_checkpoint)\n","\n","    # Initialize generator's optimizer\n","    optimizer_g = torch.optim.Adam(params=filter(lambda p: p.requires_grad, generator.parameters()),\n","                                   lr=lr)\n","\n","    # Discriminator\n","    discriminator = Discriminator(kernel_size=kernel_size_d,\n","                                  n_channels=n_channels_d,\n","                                  n_blocks=n_blocks_d,\n","                                  fc_size=fc_size_d)\n","\n","    # Initialize discriminator's optimizer\n","    optimizer_d = torch.optim.Adam(params=filter(lambda p: p.requires_grad, discriminator.parameters()),\n","                                   lr=lr)\n","\n","else:\n","    checkpoint = torch.load(checkpoint)\n","    start_epoch = checkpoint['epoch'] + 1\n","    generator = checkpoint['generator']\n","    discriminator = checkpoint['discriminator']\n","    optimizer_g = checkpoint['optimizer_g']\n","    optimizer_d = checkpoint['optimizer_d']\n","    print(\"\\nLoaded checkpoint from epoch %d.\\n\" % (checkpoint['epoch'] + 1))\n","\n","# Truncated VGG19 network to be used in the loss calculation\n","truncated_vgg19 = TruncatedVGG19(i=vgg19_i, j=vgg19_j)\n","truncated_vgg19.eval()\n","\n","# Loss functions\n","content_loss_criterion = nn.MSELoss()\n","adversarial_loss_criterion = nn.BCEWithLogitsLoss()\n","\n","# Move to default device\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","truncated_vgg19 = truncated_vgg19.to(device)\n","content_loss_criterion = content_loss_criterion.to(device)\n","adversarial_loss_criterion = adversarial_loss_criterion.to(device)\n","\n","# Custom dataloaders\n","train_dataset = SRDataset(data_folder,\n","                          split='train',\n","                          crop_size=crop_size,\n","                          scaling_factor=scaling_factor,\n","                          lr_img_type='imagenet-norm',\n","                          hr_img_type='imagenet-norm')\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers,\n","                                           pin_memory=True)\n","\n","# Total number of epochs to train for\n","epochs = int(iterations // len(train_loader) + 1)\n","\n","# Epochs\n","for epoch in range(start_epoch, epochs):\n","\n","    # At the halfway point, reduce learning rate to a tenth\n","    if epoch == int((iterations / 2) // len(train_loader) + 1):\n","        adjust_learning_rate(optimizer_g, 0.1)\n","        adjust_learning_rate(optimizer_d, 0.1)\n","\n","    \"\"\"\n","    One epoch's training.\n","    :param train_loader: train dataloader\n","    :param generator: generator\n","    :param discriminator: discriminator\n","    :param truncated_vgg19: truncated VGG19 network\n","    :param content_loss_criterion: content loss function (Mean Squared-Error loss)\n","    :param adversarial_loss_criterion: adversarial loss function (Binary Cross-Entropy loss)\n","    :param optimizer_g: optimizer for the generator\n","    :param optimizer_d: optimizer for the discriminator\n","    :param epoch: epoch number\n","    \"\"\"\n","    # Set to train mode\n","    generator.train()\n","    discriminator.train()  # training mode enables batch normalization\n","\n","    batch_time = AverageMeter()  # forward prop. + back prop. time\n","    data_time = AverageMeter()  # data loading time\n","    losses_c = AverageMeter()  # content loss\n","    losses_a = AverageMeter()  # adversarial loss in the generator\n","    losses_d = AverageMeter()  # adversarial loss in the discriminator\n","\n","    start = time.time()\n","\n","    # Batches\n","    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n","        data_time.update(time.time() - start)\n","\n","        # Move to default device\n","        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24), imagenet-normed\n","        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96), imagenet-normed\n","\n","        # GENERATOR UPDATE\n","\n","        # Generate\n","        sr_imgs = generator(lr_imgs)  # (N, 3, 96, 96), in [-1, 1]\n","        sr_imgs = convert_image(sr_imgs, source='[-1, 1]', target='imagenet-norm')  # (N, 3, 96, 96), imagenet-normed\n","\n","        # Calculate VGG feature maps for the super-resolved (SR) and high resolution (HR) images\n","        sr_imgs_in_vgg_space = truncated_vgg19(sr_imgs)\n","        hr_imgs_in_vgg_space = truncated_vgg19(hr_imgs).detach()  # detached because they're constant, targets\n","\n","        # Discriminate super-resolved (SR) images\n","        sr_discriminated = discriminator(sr_imgs)  # (N)\n","\n","        # Calculate the Perceptual loss\n","        content_loss = content_loss_criterion(sr_imgs_in_vgg_space, hr_imgs_in_vgg_space)\n","        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.ones_like(sr_discriminated))\n","        perceptual_loss = content_loss + beta * adversarial_loss\n","\n","        # Back-prop.\n","        optimizer_g.zero_grad()\n","        perceptual_loss.backward()\n","\n","        # Clip gradients, if necessary\n","        if grad_clip is not None:\n","            clip_gradient(optimizer_g, grad_clip)\n","\n","        # Update generator\n","        optimizer_g.step()\n","\n","        # Keep track of loss\n","        losses_c.update(content_loss.item(), lr_imgs.size(0))\n","        losses_a.update(adversarial_loss.item(), lr_imgs.size(0))\n","\n","        # DISCRIMINATOR UPDATE\n","\n","        # Discriminate super-resolution (SR) and high-resolution (HR) images\n","        hr_discriminated = discriminator(hr_imgs)\n","        sr_discriminated = discriminator(sr_imgs.detach())\n","        # But didn't we already discriminate the SR images earlier, before updating the generator (G)? Why not just use that here?\n","        # Because, if we used that, we'd be back-propagating (finding gradients) over the G too when backward() is called\n","        # It's actually faster to detach the SR images from the G and forward-prop again, than to back-prop. over the G unnecessarily\n","        # See FAQ section in the tutorial\n","\n","        # Binary Cross-Entropy loss\n","        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.zeros_like(sr_discriminated)) + \\\n","                           adversarial_loss_criterion(hr_discriminated, torch.ones_like(hr_discriminated))\n","\n","        # Back-prop.\n","        optimizer_d.zero_grad()\n","        adversarial_loss.backward()\n","\n","        # Clip gradients, if necessary\n","        if grad_clip is not None:\n","            clip_gradient(optimizer_d, grad_clip)\n","\n","        # Update discriminator\n","        optimizer_d.step()\n","\n","        # Keep track of loss\n","        losses_d.update(adversarial_loss.item(), hr_imgs.size(0))\n","\n","        # Keep track of batch times\n","        batch_time.update(time.time() - start)\n","\n","        # Reset start time\n","        start = time.time()\n","\n","        # Print status\n","        if i % print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]----'\n","                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})----'\n","                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})----'\n","                  'Cont. Loss {loss_c.val:.4f} ({loss_c.avg:.4f})----'\n","                  'Adv. Loss {loss_a.val:.4f} ({loss_a.avg:.4f})----'\n","                  'Disc. Loss {loss_d.val:.4f} ({loss_d.avg:.4f})'.format(epoch,\n","                                                                          i,\n","                                                                          len(train_loader),\n","                                                                          batch_time=batch_time,\n","                                                                          data_time=data_time,\n","                                                                          loss_c=losses_c,\n","                                                                          loss_a=losses_a,\n","                                                                          loss_d=losses_d))\n","\n","    # Save checkpoint\n","    torch.save({'epoch': epoch,\n","                'generator': generator,\n","                'discriminator': discriminator,\n","                'optimizer_g': optimizer_g,\n","                'optimizer_d': optimizer_d},\n","               'checkpoint_srgan.pth.tar')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.SRResNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.ConvolutionalBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.PReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.ResidualBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.SubPixelConvolutionalBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pixelshuffle.PixelShuffle' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Loaded weights from pre-trained SRResNet.\n","\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73331ac466c244c190f7dd6381827a6b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [0][0/7706]----Batch Time 3.589 (3.589)----Data Time 0.718 (0.718)----Cont. Loss 0.4312 (0.4312)----Adv. Loss 0.6433 (0.6433)----Disc. Loss 1.4048 (1.4048)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-53983f16966e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Back-prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mperceptual_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Clip gradients, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Qcy2pVZqwbm_"},"source":["## Evaluation\n","\n","Download SRGAN Pretrained Weight [here](https://drive.google.com/file/d/1_PJ1Uimbr0xrPjE8U3Q_bG7XycGgsbVo/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"g2dR2ub2zOvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634630179154,"user_tz":-540,"elapsed":15690,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"5c7e9ac2-c38c-4f30-8ee7-c4f63f3bce91"},"source":["# Data\n","data_folder = \"./\"\n","test_data_names = [\"original\"]\n","\n","# Model checkpoints\n","srgan_checkpoint = \"./checkpoint_srgan.pth.tar\"\n","srresnet_checkpoint = \"./checkpoint_srresnet.pth.tar\"\n","\n","# Load model, either the SRResNet or the SRGAN\n","#srresnet = torch.load(srresnet_checkpoint, map_location=torch.device(device))['model'].to(device)\n","#srresnet.eval()\n","#model = srresnet\n","srgan_generator = torch.load(srgan_checkpoint, map_location=torch.device(device))['generator'].to(device)\n","srgan_generator.eval()\n","model = srgan_generator\n","\n","# Evaluate\n","for test_data_name in test_data_names:\n","    print(\"\\nFor %s:\\n\" % test_data_name)\n","\n","    # Custom dataloader\n","    test_dataset = SRDataset(data_folder,\n","                             split='test',\n","                             crop_size=0,\n","                             scaling_factor=4,\n","                             lr_img_type='imagenet-norm',\n","                             hr_img_type='[-1, 1]',\n","                             test_data_name=test_data_name)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4,\n","                                              pin_memory=True)\n","\n","    # Keep track of the PSNRs and the SSIMs across batches\n","    PSNRs = AverageMeter()\n","    SSIMs = AverageMeter()\n","\n","    # Prohibit gradient computation explicitly because I had some problems with memory\n","    with torch.no_grad():\n","        # Batches\n","        for i, (lr_imgs, hr_imgs) in enumerate(test_loader):\n","            # Move to default device\n","            lr_imgs = lr_imgs.to(device)  # (batch_size (1), 3, w / 4, h / 4), imagenet-normed\n","            hr_imgs = hr_imgs.to(device)  # (batch_size (1), 3, w, h), in [-1, 1]\n","\n","            # Forward prop.\n","            sr_imgs = model(lr_imgs)  # (1, 3, w, h), in [-1, 1]\n","\n","            # Calculate PSNR and SSIM\n","            sr_imgs_y = convert_image(sr_imgs, source='[-1, 1]', target='y-channel').squeeze(\n","                0)  # (w, h), in y-channel\n","            hr_imgs_y = convert_image(hr_imgs, source='[-1, 1]', target='y-channel').squeeze(0)  # (w, h), in y-channel\n","            psnr = peak_signal_noise_ratio(hr_imgs_y.cpu().numpy(), sr_imgs_y.cpu().numpy(),\n","                                           data_range=255.)\n","            ssim = structural_similarity(hr_imgs_y.cpu().numpy(), sr_imgs_y.cpu().numpy(),\n","                                         data_range=255.)\n","            PSNRs.update(psnr, lr_imgs.size(0))\n","            SSIMs.update(ssim, lr_imgs.size(0))\n","\n","    # Print average PSNR and SSIM\n","    print('PSNR - {psnrs.avg:.3f}'.format(psnrs=PSNRs))\n","    print('SSIM - {ssims.avg:.3f}'.format(ssims=SSIMs))\n","\n","print(\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.Generator' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.SRResNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.ConvolutionalBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.PReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.ResidualBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.SubPixelConvolutionalBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pixelshuffle.PixelShuffle' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'models.Discriminator' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","For original:\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["PSNR - 26.509\n","SSIM - 0.729\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"DrxMbf0Tz0Ap"},"source":["## Visualize"]},{"cell_type":"code","metadata":{"id":"jBHg4x0Nz2Kv","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LKSGi3ndK0ysbjv7swlNCj8dDfwIyCCi"},"executionInfo":{"status":"ok","timestamp":1634630210616,"user_tz":-540,"elapsed":2786,"user":{"displayName":"Heeseung Yun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQC354IpbsjB_YJc8UOKhiXw_Yo8Jbw2M7fntW=s64","userId":"04364160322897221272"}},"outputId":"50b0fd2e-b058-4a50-9cfc-c4b28e6fe7a2"},"source":["# Model checkpoints\n","srgan_checkpoint = \"./checkpoint_srgan.pth.tar\"\n","srresnet_checkpoint = \"./checkpoint_srresnet.pth.tar\"\n","\n","# Load models\n","srresnet = torch.load(srresnet_checkpoint)['model'].to(device)\n","srresnet.eval()\n","srgan_generator = torch.load(srgan_checkpoint)['generator'].to(device)\n","srgan_generator.eval()\n","\n","\n","def visualize_sr(img, halve=False):\n","    \"\"\"\n","    Visualizes the super-resolved images from the SRResNet and SRGAN for comparison with the bicubic-upsampled image\n","    and the original high-resolution (HR) image, as done in the paper.\n","    :param img: filepath of the HR iamge\n","    :param halve: halve each dimension of the HR image to make sure it's not greater than the dimensions of your screen?\n","                  For instance, for a 2160p HR image, the LR image will be of 540p (1080p/4) resolution. On a 1080p screen,\n","                  you will therefore be looking at a comparison between a 540p LR image and a 1080p SR/HR image because\n","                  your 1080p screen can only display the 2160p SR/HR image at a downsampled 1080p. This is only an\n","                  APPARENT rescaling of 2x.\n","                  If you want to reduce HR resolution by a different extent, modify accordingly.\n","    \"\"\"\n","    # Load image, downsample to obtain low-res version\n","    hr_img = Image.open(img, mode=\"r\")\n","    hr_img = hr_img.convert('RGB')\n","    if halve:\n","        hr_img = hr_img.resize((int(hr_img.width / 2), int(hr_img.height / 2)),\n","                               Image.LANCZOS)\n","    lr_img = hr_img.resize((int(hr_img.width / 4), int(hr_img.height / 4)),\n","                           Image.BICUBIC)\n","\n","    # Bicubic Upsampling\n","    bicubic_img = lr_img.resize((hr_img.width, hr_img.height), Image.BICUBIC)\n","\n","    # Super-resolution (SR) with SRResNet\n","    sr_img_srresnet = srresnet(convert_image(lr_img, source='pil', target='imagenet-norm').unsqueeze(0).to(device))\n","    sr_img_srresnet = sr_img_srresnet.squeeze(0).cpu().detach()\n","    sr_img_srresnet = convert_image(sr_img_srresnet, source='[-1, 1]', target='pil')\n","\n","    # Super-resolution (SR) with SRGAN\n","    sr_img_srgan = srgan_generator(convert_image(lr_img, source='pil', target='imagenet-norm').unsqueeze(0).to(device))\n","    sr_img_srgan = sr_img_srgan.squeeze(0).cpu().detach()\n","    sr_img_srgan = convert_image(sr_img_srgan, source='[-1, 1]', target='pil')\n","\n","    # Create grid\n","    margin = 40\n","    grid_img = Image.new('RGB', (2 * hr_img.width + 3 * margin, 2 * hr_img.height + 3 * margin), (255, 255, 255))\n","\n","    # Font\n","    draw = ImageDraw.Draw(grid_img)\n","    try:\n","        font = ImageFont.truetype(\"calibril.ttf\", size=23)\n","        # It will also look for this file in your OS's default fonts directory, where you may have the Calibri Light font installed if you have MS Office\n","        # Otherwise, use any TTF font of your choice\n","    except OSError:\n","        print(\n","            \"Defaulting to a terrible font. To use a font of your choice, include the link to its TTF file in the function.\")\n","        font = ImageFont.load_default()\n","\n","    # Place bicubic-upsampled image\n","    grid_img.paste(bicubic_img, (margin, margin))\n","    text_size = font.getsize(\"Bicubic\")\n","    draw.text(xy=[margin + bicubic_img.width / 2 - text_size[0] / 2, margin - text_size[1] - 5], text=\"Bicubic\",\n","              font=font,\n","              fill='black')\n","\n","    # Place SRResNet image\n","    grid_img.paste(sr_img_srresnet, (2 * margin + bicubic_img.width, margin))\n","    text_size = font.getsize(\"SRResNet\")\n","    draw.text(\n","        xy=[2 * margin + bicubic_img.width + sr_img_srresnet.width / 2 - text_size[0] / 2, margin - text_size[1] - 5],\n","        text=\"SRResNet\", font=font, fill='black')\n","\n","    # Place SRGAN image\n","    grid_img.paste(sr_img_srgan, (margin, 2 * margin + sr_img_srresnet.height))\n","    text_size = font.getsize(\"SRGAN\")\n","    draw.text(\n","        xy=[margin + bicubic_img.width / 2 - text_size[0] / 2, 2 * margin + sr_img_srresnet.height - text_size[1] - 5],\n","        text=\"SRGAN\", font=font, fill='black')\n","\n","    # Place original HR image\n","    grid_img.paste(hr_img, (2 * margin + bicubic_img.width, 2 * margin + sr_img_srresnet.height))\n","    text_size = font.getsize(\"Original HR\")\n","    draw.text(xy=[2 * margin + bicubic_img.width + sr_img_srresnet.width / 2 - text_size[0] / 2,\n","                  2 * margin + sr_img_srresnet.height - text_size[1] - 1], text=\"Original HR\", font=font, fill='black')\n","\n","    # Display grid\n","    grid_img.show()\n","    display(grid_img)\n","\n","    return grid_img\n","\n","\n","if __name__ == '__main__':\n","    # TODO: Try with your own image\n","    grid_img = visualize_sr(\"./Set14/original/baboon.png\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}