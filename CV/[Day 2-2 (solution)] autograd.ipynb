{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"[Day 2-2 (solution)] autograd.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/009cea8b0f40dfcb55e3280f73b06cc2/autograd_tutorial.ipynb","timestamp":1606326591953}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"iklGeXOmguf0","executionInfo":{"status":"ok","timestamp":1652770305128,"user_tz":-540,"elapsed":10,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p0XzyD4pguf0"},"source":["\n","Autograd: Automatic Differentiation\n","===================================\n","\n","Central to all neural networks in PyTorch is the ``autograd`` package.\n","\n","The ``autograd`` package provides automatic differentiation for all operations\n","on Tensors. \n","\n","\n","Tensor\n","--------\n","\n","``torch.Tensor`` is the central class of the package. If you set its attribute\n","``.requires_grad`` as ``True``, it starts to track all operations on it. When\n","you finish your computation you can call ``.backward()`` and have all the\n","gradients computed automatically. The gradient for this tensor will be\n","accumulated into ``.grad`` attribute.\n","\n","To stop a tensor from tracking history, you can call ``.detach()`` to detach\n","it from the computation history, and to prevent future computation from being\n","tracked.\n","\n","To prevent tracking history (and using memory), you can also wrap the code block\n","in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n","model because the model may have trainable parameters with\n","``requires_grad=True``, but for which we don't need the gradients.\n","\n","If you want to compute the derivatives, you can call ``.backward()`` on\n","a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\n","data), you don’t need to specify any arguments to ``backward()``,\n","however if it has more elements, you need to specify a ``gradient``\n","argument that is a tensor of matching shape.\n","\n"]},{"cell_type":"code","metadata":{"id":"gSMMd57Nguf0","executionInfo":{"status":"ok","timestamp":1652770309838,"user_tz":-540,"elapsed":4245,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}}},"source":["import torch"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9EPpwIcvguf0"},"source":["Create a tensor and set ``requires_grad=True`` to track computation with it\n","\n"]},{"cell_type":"code","metadata":{"id":"NN81Q1X1guf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310405,"user_tz":-540,"elapsed":580,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"0a06ec36-16e6-48e9-fff7-de843dcff043"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"U2a0tsUFguf0"},"source":["Do a tensor operation:\n","\n"]},{"cell_type":"code","metadata":{"id":"797qSzTfguf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310406,"user_tz":-540,"elapsed":33,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"91c7cb25-baed-499d-8d47-c84d9af44c3a"},"source":["y = x + 2\n","print(y)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"FMCThxZwguf0"},"source":["``y`` was created as a result of an operation, so it has a ``grad_fn``.\n","\n"]},{"cell_type":"code","metadata":{"id":"UWadXkvbguf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310408,"user_tz":-540,"elapsed":33,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"ce37c3d3-c9ed-4554-bbe8-1dc66267ac57"},"source":["print(y.grad_fn)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<AddBackward0 object at 0x7fc45c62aa10>\n"]}]},{"cell_type":"markdown","metadata":{"id":"7mTqEndrguf0"},"source":["Do more operations on ``y``\n","\n"]},{"cell_type":"code","metadata":{"id":"8gyn0Sm6guf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310409,"user_tz":-540,"elapsed":31,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"b703e3c0-43dc-4421-961e-b33ed8deada0"},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"mqbbSNlgguf0"},"source":["``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n","flag in-place. The input flag defaults to ``False`` if not given.\n","\n"]},{"cell_type":"code","metadata":{"id":"ZYPQ5m9tguf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310409,"user_tz":-540,"elapsed":28,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"65b9371e-bb22-40a9-f0d7-19770e8927f3"},"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","True\n","<SumBackward0 object at 0x7fc3a0828390>\n"]}]},{"cell_type":"markdown","metadata":{"id":"hVRmDA5Bguf0"},"source":["Gradients\n","---------\n","Let's backprop now.\n","Because ``out`` contains a single scalar, ``out.backward()`` is\n","equivalent to ``out.backward(torch.tensor(1.))``.\n","\n"]},{"cell_type":"code","metadata":{"id":"tOGGmG1bguf0","executionInfo":{"status":"ok","timestamp":1652770310410,"user_tz":-540,"elapsed":27,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}}},"source":["out.backward()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7_1Rjqqguf0"},"source":["Print gradients d(out)/dx\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"gB1zU63Bguf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310411,"user_tz":-540,"elapsed":27,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"79647109-b0c2-4e88-86d3-9672a5ef2047"},"source":["print(x.grad)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"lGhzM9AJguf0"},"source":["You should have got a matrix of ``4.5``. Let’s call the ``out``\n","*Tensor* “$o$”.\n","We have that $o = \\frac{1}{4}\\sum_i z_i$,\n","$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n","Therefore,\n","$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n","$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L3V3qVguguf1"},"source":["Now let's take a look at an example of vector-Jacobian product:\n","\n"]},{"cell_type":"code","metadata":{"id":"jyXY1TVcguf1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310412,"user_tz":-540,"elapsed":25,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"8e9a9b83-692f-4e70-e016-c083bb466075"},"source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-937.9263, -801.6949,  801.0406], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"f-D4dRScguf1"},"source":["Now in this case ``y`` is no longer a scalar. ``torch.autograd``\n","could not compute the full Jacobian directly, but if we just\n","want the vector-Jacobian product, simply pass the vector to\n","``backward`` as argument:\n","\n"]},{"cell_type":"code","metadata":{"id":"YO9Hr30Gguf1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310412,"user_tz":-540,"elapsed":23,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"742f613c-774b-497a-d4e1-33ae9103fcf1"},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","\n","print(x.grad)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"]}]},{"cell_type":"markdown","metadata":{"id":"bwsmI_qsguf1"},"source":["You can also stop autograd from tracking history on Tensors\n","with ``.requires_grad=True`` either by wrapping the code block in\n","``with torch.no_grad():``\n","\n"]},{"cell_type":"code","metadata":{"id":"HtMH2wy-guf1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310413,"user_tz":-540,"elapsed":22,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"289d13b0-8410-4ffd-9b94-249f7bbde690"},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","\tprint((x ** 2).requires_grad)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","True\n","False\n"]}]},{"cell_type":"markdown","metadata":{"id":"2Z5uyZVTguf1"},"source":["Or by using ``.detach()`` to get a new Tensor with the same\n","content but that does not require gradients:\n","\n"]},{"cell_type":"code","metadata":{"id":"g8ytMZWMguf1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310414,"user_tz":-540,"elapsed":20,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"23b07ba9-89fb-403b-d5cd-518df2295abf"},"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.eq(y).all())"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n","tensor(True)\n"]}]},{"cell_type":"code","metadata":{"id":"FXH2jcc8xfI0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652770310414,"user_tz":-540,"elapsed":18,"user":{"displayName":"Heeseung Yun","userId":"04364160322897221272"}},"outputId":"0c75b906-635b-4b53-856d-c50a25e3006f"},"source":["# TODO: Compute the Jacobian-vector product J(x)v, \n","# where J(x) is the Jacobian of the function f: R^3->R^3, f(x) = x^3 + x^2 + exp(x) at x, \n","# and both x and v are random-sampled vector of size 3\n","x = torch.randn(3, requires_grad=True)\n","v = torch.randn(3)\n","y = x ** 3 + x ** 2 + torch.exp(x)\n","y.backward(v)\n","print((3*x**2 + 2*x + torch.exp(x)) * v)\n","print(x.grad)\n","\n","x = torch.randn(1, requires_grad=True)\n","y = x ** 3 + x ** 2 + torch.exp(x)\n","y.backward()\n","print(3*x**2 + 2*x + torch.exp(x))\n","print(x.grad)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.6832, 17.1128, -0.3197], grad_fn=<MulBackward0>)\n","tensor([ 1.6832, 17.1128, -0.3197])\n","tensor([1.1743], grad_fn=<AddBackward0>)\n","tensor([1.1743])\n"]}]}]}