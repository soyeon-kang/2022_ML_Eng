{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Day 5-1] YOLOv3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"u5orS8TtQZFJ"},"source":["# YOLO v3\n","In this notebook, we will go through [a YOLO v3 PyTorch implementation](https://github.com/eriklindernoren/PyTorch-YOLOv3) and learn how YOLO v3 is actually implemented in code.\n","\n","![yolo](https://blog.paperspace.com/content/images/size/w2000/2018/04/maxresdefault-p1.jpg)\n","\n","Credit for the explanation in this notebook is due to this [post](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)."]},{"cell_type":"markdown","metadata":{"id":"0BbFGMwEVRi5"},"source":["# 1. Overall Architecture\n","YOLO makes use of only convolutional layers, making it **a fully convolutional network (FCN)**. It has 75 convolutional layers, with skip connections and upsampling layers. No form of pooling is used, and a convolutional layer with stride 2 is used to downsample the feature maps. This helps in preventing loss of low-level features often attributed to pooling.\n","\n","The network downsamples the image by a factor called the stride of the network. For example, if the stride of the network is 32, then an input image of size 416 x 416 will yield an output of size 13 x 13.\n","\n","# 2. Output of the network\n","\n","![dog](https://blog.paperspace.com/content/images/2018/04/yolo-5.png)\n","\n","Depth-wise, we have **(B x (5 + C)) entries in the feature map**. B represents the number of bounding boxes each cell can predict. According to the paper, each of these B bounding boxes may specialize in detecting a certain kind of object. Each of the bounding boxes have 5 + C attributes, which describe the center coordinates, the dimensions, the objectness score and C class confidences for each bounding box. YOLO v3 predicts 3 bounding boxes for every cell.\n","\n","You expect **each cell of the feature map to predict an object** through one of it's bounding boxes **if the center of the object falls in the receptive field of that cell**. (Receptive field is the region of the input image visible to the cell. Refer to the link on convolutional neural networks for further clarification)."]},{"cell_type":"markdown","metadata":{"id":"MsSfgqnmpilc"},"source":["# 3. Details of the Predictions\n","The following formulae describe how the network output is transformed to obtain bounding box predictions.\n","\n","## YOLO Equations\n","bx, by, bw, bh are the x,y center co-ordinates, width and height of our prediction. tx, ty, tw, th is what the network outputs. cx and cy are the top-left co-ordinates of the grid. pw and ph are anchors dimensions for the box.\n","\n","![box coordinates](https://blog.paperspace.com/content/images/2018/04/Screen-Shot-2018-04-10-at-3.18.08-PM.png)\n","\n","## Center Coordinates\n","Notice we are running our center coordinates prediction through a sigmoid function. This forces the value of the output to be between 0 and 1. Why should this be the case? Bear with me.\n","\n","Normally, YOLO doesn't predict the absolute coordinates of the bounding box's center. It predicts offsets which are:\n","\n","Relative to the top left corner of the grid cell which is predicting the object.\n","\n","Normalised by the dimensions of the cell from the feature map, which is, 1.\n","\n","For example, consider the case of our dog image. If the prediction for center is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x 13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).\n","\n","But wait, what happens if the predicted x,y co-ordinates are greater than one, say (1.2, 0.7). This means center lies at (7.2, 6.7). Notice the center now lies in cell just right to our red cell, or the 8th cell in the 7th row. This breaks theory behind YOLO because if we postulate that the red box is responsible for predicting the dog, the center of the dog must lie in the red cell, and not in the one beside it.\n","\n","Therefore, to remedy this problem, the output is passed through a sigmoid function, which squashes the output in a range from 0 to 1, effectively keeping the center in the grid which is predicting.\n","\n","## Dimensions of the Bounding Box\n","\n","![dimsof](https://blog.paperspace.com/content/images/2018/04/yolo-regression-1.png)\n","\n","The dimensions of the bounding box are predicted by applying a log-space transform to the output and then multiplying with an anchor.\n","\n","How the detector output is transformed to give the final prediction. Image Credits. http://christopher5106.github.io/\n","\n","The resultant predictions, bw and bh, are normalised by the height and width of the image. (Training labels are chosen this way). So, if the predictions bx and by for the box containing the dog are (0.3, 0.8), then the actual width and height on 13 x 13 feature map is (13 x 0.3, 13 x 0.8).\n","\n","## Objectness Score\n","Object score represents the probability that an object is contained inside a bounding box. It should be nearly 1 for the red and the neighboring grids, whereas almost 0 for, say, the grid at the corners.\n","\n","The objectness score is also passed through a sigmoid, as it is to be interpreted as a probability.\n","\n","## Class Confidences\n","Class confidences represent the probabilities of the detected object belonging to a particular class (Dog, cat, banana, car etc). Before v3, YOLO used to softmax the class scores.\n","\n","However, that design choice has been dropped in v3, and authors have opted for using sigmoid instead. The reason is that Softmaxing class scores assume that the classes are mutually exclusive. In simple words, if an object belongs to one class, then it's guaranteed it cannot belong to another class. This is true for COCO database on which we will base our detector."]},{"cell_type":"markdown","metadata":{"id":"2Wg2fNOnX5zd"},"source":["# 4. Prediction across different scales.\n","\n","![scales](https://blog.paperspace.com/content/images/2018/04/yolo_Scales-1.png)\n","\n","**YOLO v3 makes prediction across 3 different scales.** The detection layer is used make detection at feature maps of three different sizes, having strides 32, 16, 8 respectively. This means, with an input of 416 x 416, we **make detections on scales 13 x 13, 26 x 26 and 52 x 52**.\n","\n","The network downsamples the input image until the first detection layer, where **a detection is made using feature maps of a layer with stride 32**. Further, layers are **upsampled by a factor of 2 and concatenated with feature maps of a previous layers** having identical feature map sizes. Another detection is now made at layer with stride 16. The same upsampling procedure is repeated, and a final detection is made at the layer of stride 8.\n","\n","At each scale, **each cell predicts 3 bounding boxes using 3 anchors, making the total number of anchors used 9**. (The anchors are different for different scales)"]},{"cell_type":"markdown","metadata":{"id":"2GCzkGiDYz2w"},"source":["# 5. Output Processing\n","For an image of size 416 x 416, YOLO predicts ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = 10647 bounding boxes. However, in case of our image, there's only one object, a dog. How do we reduce the detections from 10647 to 1?\n","\n","### 1. Thresholding by Object Confidence\n","First, we filter boxes based on their objectness score. Generally, boxes having scores below a threshold are ignored.\n","\n","### 2. Non-maximum Suppression (NMS)\n","![nms](https://blog.paperspace.com/content/images/2018/04/NMS-1.png)\n","\n","NMS intends to cure the problem of multiple detections of the same image. For example, all the 3 bounding boxes of the red grid cell may detect a box or the adjacent cells may detect the same object."]},{"cell_type":"markdown","metadata":{"id":"45QnxB0XPfwk"},"source":["# Download the codebase"]},{"cell_type":"code","metadata":{"id":"pLzP---DPeWk"},"source":["!git clone https://github.com/eriklindernoren/PyTorch-YOLOv3\n","!cd PyTorch-YOLOv3 && git checkout 83c3d\n","!cd ..\n","!cp -r PyTorch-YOLOv3/* .\n","!rm -rf PyTorch-YOLOv3/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WrCfNw-bnJx"},"source":["!cd weights && bash download_weights.sh\n","\n","# Downloading COCO dataset takes about 5 hours.\n","# !cd data && bash get_coco_dataset.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PEUluLyJT3vu"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"P-RmdEQOQrsy"},"source":["import os\n","import sys\n","import time\n","import datetime\n","import argparse\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision import transforms\n","import torch.optim as optim\n","\n","from utils.logger import *\n","from utils.datasets import *\n","from utils.parse_config import *\n","from utils.utils import *\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.ticker import NullLocator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXGgdj-dT2cp"},"source":["# YOLO Layers"]},{"cell_type":"code","metadata":{"id":"uV3TZC-NP1OB"},"source":["class YOLOLayer(nn.Module):\n","    \"\"\"Detection layer\"\"\"\n","\n","    def __init__(self, anchors, num_classes, img_dim=416):\n","        super(YOLOLayer, self).__init__()\n","        self.anchors = anchors\n","        self.num_anchors = len(anchors)\n","        self.num_classes = num_classes\n","        self.ignore_thres = 0.5\n","        self.mse_loss = nn.MSELoss()\n","        self.bce_loss = nn.BCELoss()\n","        self.obj_scale = 1\n","        self.noobj_scale = 100\n","        self.metrics = {}\n","        self.img_dim = img_dim\n","        self.grid_size = 0  # grid size\n","\n","    def compute_grid_offsets(self, grid_size, cuda=True):\n","        self.grid_size = grid_size\n","        g = self.grid_size\n","        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","        self.stride = self.img_dim / self.grid_size\n","        # Calculate offsets for each grid\n","        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor) \n","        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n","        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n","        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n","        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n","\n","    def forward(self, x, targets=None, img_dim=None):\n","        # Tensors for cuda support\n","        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n","        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n","        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n","\n","        self.img_dim = img_dim\n","        num_samples = x.size(0)\n","        grid_size = x.size(2)\n","\n","        prediction = (\n","            # reshape to BATCH x B x (C + 5) x F x F\n","            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n","            .permute(0, 1, 3, 4, 2)\n","            .contiguous()\n","        )\n","\n","        x = torch.sigmoid(prediction[..., 0])  # Center x\n","        y = torch.sigmoid(prediction[..., 1])  # Center y\n","        w = prediction[..., 2]  # Width\n","        h = prediction[..., 3]  # Height\n","\n","        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n","        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n","\n","        # If grid size does not match current we compute new offsets\n","        if grid_size != self.grid_size:\n","            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n","\n","        # Add offset and scale with anchors\n","        pred_boxes = FloatTensor(prediction[..., :4].shape)\n","        pred_boxes[..., 0] = x.data + self.grid_x\n","        pred_boxes[..., 1] = y.data + self.grid_y\n","        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n","        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n","\n","        output = torch.cat(\n","            (\n","                pred_boxes.view(num_samples, -1, 4) * self.stride,\n","                pred_conf.view(num_samples, -1, 1),\n","                pred_cls.view(num_samples, -1, self.num_classes),\n","            ),\n","            -1,\n","        )\n","\n","        if targets is None:\n","            return output, 0\n","        else:\n","            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n","                pred_boxes=pred_boxes,\n","                pred_cls=pred_cls,\n","                target=targets,\n","                anchors=self.scaled_anchors,\n","                ignore_thres=self.ignore_thres,\n","            )\n","\n","            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n","            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n","            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n","            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n","            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n","            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n","            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n","            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n","            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n","            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n","\n","            # Metrics\n","            cls_acc = 100 * class_mask[obj_mask].mean()\n","            conf_obj = pred_conf[obj_mask].mean()\n","            conf_noobj = pred_conf[noobj_mask].mean()\n","            conf50 = (pred_conf > 0.5).float()\n","            iou50 = (iou_scores > 0.5).float()\n","            iou75 = (iou_scores > 0.75).float()\n","            detected_mask = conf50 * class_mask * tconf\n","            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n","            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n","            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n","\n","            self.metrics = {\n","                \"loss\": to_cpu(total_loss).item(),\n","                \"x\": to_cpu(loss_x).item(),\n","                \"y\": to_cpu(loss_y).item(),\n","                \"w\": to_cpu(loss_w).item(),\n","                \"h\": to_cpu(loss_h).item(),\n","                \"conf\": to_cpu(loss_conf).item(),\n","                \"cls\": to_cpu(loss_cls).item(),\n","                \"cls_acc\": to_cpu(cls_acc).item(),\n","                \"recall50\": to_cpu(recall50).item(),\n","                \"recall75\": to_cpu(recall75).item(),\n","                \"precision\": to_cpu(precision).item(),\n","                \"conf_obj\": to_cpu(conf_obj).item(),\n","                \"conf_noobj\": to_cpu(conf_noobj).item(),\n","                \"grid_size\": grid_size,\n","            }\n","\n","            return output, total_loss\n","\n","\n","class Upsample(nn.Module):\n","    \"\"\" nn.Upsample is deprecated \"\"\"\n","\n","    def __init__(self, scale_factor, mode=\"nearest\"):\n","        super(Upsample, self).__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","\n","    def forward(self, x):\n","        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n","        return x\n","\n","\n","class EmptyLayer(nn.Module):\n","    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n","\n","    def __init__(self):\n","        super(EmptyLayer, self).__init__()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yi3k2OoSTocA"},"source":["# Architecture Config Reader"]},{"cell_type":"code","metadata":{"id":"sMHu-BUMTtcs"},"source":["!cat config/yolov3.cfg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcIirZcGTmu1"},"source":["def create_modules(module_defs):\n","    \"\"\"\n","    Constructs module list of layer blocks from module configuration in module_defs\n","    \"\"\"\n","    hyperparams = module_defs.pop(0)\n","    output_filters = [int(hyperparams[\"channels\"])]\n","    module_list = nn.ModuleList()\n","    for module_i, module_def in enumerate(module_defs):\n","        modules = nn.Sequential()\n","\n","        if module_def[\"type\"] == \"convolutional\":\n","            bn = int(module_def[\"batch_normalize\"])\n","            filters = int(module_def[\"filters\"])\n","            kernel_size = int(module_def[\"size\"])\n","            pad = (kernel_size - 1) // 2\n","            modules.add_module(\n","                f\"conv_{module_i}\",\n","                nn.Conv2d(\n","                    in_channels=output_filters[-1],\n","                    out_channels=filters,\n","                    kernel_size=kernel_size,\n","                    stride=int(module_def[\"stride\"]),\n","                    padding=pad,\n","                    bias=not bn,\n","                ),\n","            )\n","            if bn:\n","                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n","            if module_def[\"activation\"] == \"leaky\":\n","                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n","\n","        elif module_def[\"type\"] == \"maxpool\":\n","            kernel_size = int(module_def[\"size\"])\n","            stride = int(module_def[\"stride\"])\n","            if kernel_size == 2 and stride == 1:\n","                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n","            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n","            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n","\n","        elif module_def[\"type\"] == \"upsample\":\n","            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n","            modules.add_module(f\"upsample_{module_i}\", upsample)\n","\n","        elif module_def[\"type\"] == \"route\":\n","            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n","            filters = sum([output_filters[1:][i] for i in layers])\n","            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n","\n","        elif module_def[\"type\"] == \"shortcut\":\n","            filters = output_filters[1:][int(module_def[\"from\"])]\n","            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n","\n","        elif module_def[\"type\"] == \"yolo\":\n","            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n","            # Extract anchors\n","            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n","            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n","            anchors = [anchors[i] for i in anchor_idxs]\n","            num_classes = int(module_def[\"classes\"])\n","            img_size = int(hyperparams[\"height\"])\n","            # Define detection layer\n","            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n","            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n","        # Register module list and number of output filters\n","        module_list.append(modules)\n","        output_filters.append(filters)\n","\n","    return hyperparams, module_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTi4yuQXTdO-"},"source":["# YOLOv3 Model"]},{"cell_type":"code","metadata":{"id":"Ql7rii5RREpU"},"source":["class Darknet(nn.Module):\n","    \"\"\"YOLOv3 object detection model\"\"\"\n","\n","    def __init__(self, config_path, img_size=416):\n","        super(Darknet, self).__init__()\n","        self.module_defs = parse_model_config(config_path)\n","        self.hyperparams, self.module_list = create_modules(self.module_defs)\n","        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n","        self.img_size = img_size\n","        self.seen = 0\n","        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n","\n","    def forward(self, x, targets=None):\n","        img_dim = x.shape[2]\n","        loss = 0\n","        layer_outputs, yolo_outputs = [], []\n","        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n","            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n","                x = module(x)\n","            elif module_def[\"type\"] == \"route\":\n","                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n","            elif module_def[\"type\"] == \"shortcut\":\n","                layer_i = int(module_def[\"from\"])\n","                x = layer_outputs[-1] + layer_outputs[layer_i]\n","            elif module_def[\"type\"] == \"yolo\":\n","                x, layer_loss = module[0](x, targets, img_dim)\n","                loss += layer_loss\n","                yolo_outputs.append(x)\n","            layer_outputs.append(x)\n","        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n","        return yolo_outputs if targets is None else (loss, yolo_outputs)\n","\n","    def load_darknet_weights(self, weights_path):\n","        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n","\n","        # Open the weights file\n","        with open(weights_path, \"rb\") as f:\n","            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n","            self.header_info = header  # Needed to write header when saving weights\n","            self.seen = header[3]  # number of images seen during training\n","            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n","\n","        # Establish cutoff for loading backbone weights\n","        cutoff = None\n","        if \"darknet53.conv.74\" in weights_path:\n","            cutoff = 75\n","\n","        ptr = 0\n","        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n","            if i == cutoff:\n","                break\n","            if module_def[\"type\"] == \"convolutional\":\n","                conv_layer = module[0]\n","                if module_def[\"batch_normalize\"]:\n","                    # Load BN bias, weights, running mean and running variance\n","                    bn_layer = module[1]\n","                    num_b = bn_layer.bias.numel()  # Number of biases\n","                    # Bias\n","                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n","                    bn_layer.bias.data.copy_(bn_b)\n","                    ptr += num_b\n","                    # Weight\n","                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n","                    bn_layer.weight.data.copy_(bn_w)\n","                    ptr += num_b\n","                    # Running Mean\n","                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n","                    bn_layer.running_mean.data.copy_(bn_rm)\n","                    ptr += num_b\n","                    # Running Var\n","                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n","                    bn_layer.running_var.data.copy_(bn_rv)\n","                    ptr += num_b\n","                else:\n","                    # Load conv. bias\n","                    num_b = conv_layer.bias.numel()\n","                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n","                    conv_layer.bias.data.copy_(conv_b)\n","                    ptr += num_b\n","                # Load conv. weights\n","                num_w = conv_layer.weight.numel()\n","                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n","                conv_layer.weight.data.copy_(conv_w)\n","                ptr += num_w\n","\n","    def save_darknet_weights(self, path, cutoff=-1):\n","        \"\"\"\n","            @:param path    - path of the new weights file\n","            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n","        \"\"\"\n","        fp = open(path, \"wb\")\n","        self.header_info[3] = self.seen\n","        self.header_info.tofile(fp)\n","\n","        # Iterate through layers\n","        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n","            if module_def[\"type\"] == \"convolutional\":\n","                conv_layer = module[0]\n","                # If batch norm, load bn first\n","                if module_def[\"batch_normalize\"]:\n","                    bn_layer = module[1]\n","                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n","                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n","                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n","                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n","                # Load conv bias\n","                else:\n","                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n","                # Load conv weights\n","                conv_layer.weight.data.cpu().numpy().tofile(fp)\n","\n","        fp.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGUBeqYZcCCj"},"source":["# Training (Optional)"]},{"cell_type":"code","metadata":{"id":"9Ar6joohcB3o"},"source":["import sys\n","sys.argv=['']\n","del sys\n","\n","import tensorflow as tf\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--epochs\", type=int, default=100, help=\"number of epochs\")\n","parser.add_argument(\"--batch_size\", type=int, default=8, help=\"size of each image batch\")\n","parser.add_argument(\"--gradient_accumulations\", type=int, default=2, help=\"number of gradient accums before step\")\n","parser.add_argument(\"--model_def\", type=str, default=\"config/yolov3.cfg\", help=\"path to model definition file\")\n","parser.add_argument(\"--data_config\", type=str, default=\"config/coco.data\", help=\"path to data config file\")\n","parser.add_argument(\"--pretrained_weights\", type=str, help=\"if specified starts from checkpoint model\")\n","parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n","parser.add_argument(\"--img_size\", type=int, default=416, help=\"size of each image dimension\")\n","parser.add_argument(\"--checkpoint_interval\", type=int, default=1, help=\"interval between saving model weights\")\n","parser.add_argument(\"--evaluation_interval\", type=int, default=1, help=\"interval evaluations on validation set\")\n","parser.add_argument(\"--compute_map\", default=False, help=\"if True computes mAP every tenth batch\")\n","parser.add_argument(\"--multiscale_training\", default=True, help=\"allow for multi-scale training\")\n","opt = parser.parse_args()\n","print(opt)\n","\n","logger = tf.summary.create_file_writer('logs')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","os.makedirs(\"output\", exist_ok=True)\n","os.makedirs(\"checkpoints\", exist_ok=True)\n","\n","# Get data configuration\n","data_config = parse_data_config(opt.data_config)\n","train_path = data_config[\"train\"]\n","valid_path = data_config[\"valid\"]\n","class_names = load_classes(data_config[\"names\"])\n","\n","# Initiate model\n","model = Darknet(opt.model_def).to(device)\n","model.apply(weights_init_normal)\n","\n","# If specified we start from checkpoint\n","if opt.pretrained_weights:\n","    if opt.pretrained_weights.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(\"./data/best_model.pth\"))\n","    else:\n","        model.load_darknet_weights(opt.pretrained_weights)\n","\n","# Get dataloader\n","dataset = ListDataset(train_path, augment=True, multiscale=opt.multiscale_training)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=opt.batch_size,\n","    shuffle=True,\n","    num_workers=opt.n_cpu,\n","    pin_memory=True,\n","    collate_fn=dataset.collate_fn,\n",")\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","metrics = [\n","    \"grid_size\",\n","    \"loss\",\n","    \"x\",\n","    \"y\",\n","    \"w\",\n","    \"h\",\n","    \"conf\",\n","    \"cls\",\n","    \"cls_acc\",\n","    \"recall50\",\n","    \"recall75\",\n","    \"precision\",\n","    \"conf_obj\",\n","    \"conf_noobj\",\n","]\n","\n","for epoch in range(opt.epochs):\n","    model.train()\n","    start_time = time.time()\n","    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n","        batches_done = len(dataloader) * epoch + batch_i\n","\n","        imgs = Variable(imgs.to(device))\n","        targets = Variable(targets.to(device), requires_grad=False)\n","\n","        loss, outputs = model(imgs, targets)\n","        loss.backward()\n","\n","        if batches_done % opt.gradient_accumulations:\n","            # Accumulates gradient before each step\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # ----------------\n","        #   Log progress\n","        # ----------------\n","\n","        log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, opt.epochs, batch_i, len(dataloader))\n","\n","        metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n","\n","        # Log metrics at each YOLO layer\n","        for i, metric in enumerate(metrics):\n","            formats = {m: \"%.6f\" for m in metrics}\n","            formats[\"grid_size\"] = \"%2d\"\n","            formats[\"cls_acc\"] = \"%.2f%%\"\n","            row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n","            metric_table += [[metric, *row_metrics]]\n","\n","            # Tensorboard logging\n","            tensorboard_log = []\n","            for j, yolo in enumerate(model.yolo_layers):\n","                for name, metric in yolo.metrics.items():\n","                    if name != \"grid_size\":\n","                        tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n","            tensorboard_log += [(\"loss\", loss.item())]\n","            # logger.list_of_scalars_summary(tensorboard_log, batches_done)\n","\n","        log_str += AsciiTable(metric_table).table\n","        log_str += f\"\\nTotal loss {loss.item()}\"\n","\n","        # Determine approximate time left for epoch\n","        epoch_batches_left = len(dataloader) - (batch_i + 1)\n","        time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n","        log_str += f\"\\n---- ETA {time_left}\"\n","\n","        print(log_str)\n","\n","        model.seen += imgs.size(0)\n","\n","    if epoch % opt.evaluation_interval == 0:\n","        print(\"\\n---- Evaluating Model ----\")\n","        # Evaluate the model on the validation set\n","        precision, recall, AP, f1, ap_class = evaluate(\n","            model,\n","            path=valid_path,\n","            iou_thres=0.5,\n","            conf_thres=0.5,\n","            nms_thres=0.5,\n","            img_size=opt.img_size,\n","            batch_size=8,\n","        )\n","        evaluation_metrics = [\n","            (\"val_precision\", precision.mean()),\n","            (\"val_recall\", recall.mean()),\n","            (\"val_mAP\", AP.mean()),\n","            (\"val_f1\", f1.mean()),\n","        ]\n","        # logger.list_of_scalars_summary(evaluation_metrics, epoch)\n","\n","        # Print class APs and mAP\n","        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n","        for i, c in enumerate(ap_class):\n","            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n","        print(AsciiTable(ap_table).table)\n","        print(f\"---- mAP {AP.mean()}\")\n","\n","    if epoch % opt.checkpoint_interval == 0:\n","        torch.save(model.state_dict(), f\"checkpoints/yolov3_ckpt_%d.pth\" % epoch)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oppZR6pncF3g"},"source":["# Test the YOLOv3"]},{"cell_type":"code","metadata":{"id":"cCsYtwl4e1Eu"},"source":["IMAGE_FOLDER = 'data/samples'\n","MODEL_DEF = 'config/yolov3.cfg'\n","WEIGHTS_PATH = 'weights/yolov3.weights'\n","CLASS_PATH = 'data/coco.names'\n","CONF_THRES = 0.8\n","NMS_THRES = 0.4\n","BATCH_SIZE = 1\n","N_CPU = 0\n","IMG_SIZE = 416\n","CHECKPOINT_MODEL = ''\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","os.makedirs(\"output\", exist_ok=True)\n","\n","# Set up model\n","model = Darknet(MODEL_DEF, img_size=IMG_SIZE).to(device)\n","\n","if WEIGHTS_PATH.endswith(\".weights\"):\n","    # Load darknet weights\n","    model.load_darknet_weights(WEIGHTS_PATH)\n","else:\n","    # Load checkpoint weights\n","    model.load_state_dict(torch.load(WEIGHTS_PATH))\n","\n","model.eval()  # Set in evaluation mode\n","\n","dataloader = DataLoader(\n","    ImageFolder(IMAGE_FOLDER, img_size=IMG_SIZE),\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=N_CPU,\n",")\n","\n","classes = load_classes(CLASS_PATH)  # Extracts class labels from file\n","\n","Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n","\n","imgs = []  # Stores image paths\n","img_detections = []  # Stores detections for each image index\n","\n","print(\"\\nPerforming object detection:\")\n","prev_time = time.time()\n","for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n","    # Configure input\n","    input_imgs = Variable(input_imgs.type(Tensor))\n","\n","    # Get detections\n","    with torch.no_grad():\n","        detections = model(input_imgs)\n","        detections = non_max_suppression(detections, CONF_THRES, NMS_THRES)\n","\n","    # Log progress\n","    current_time = time.time()\n","    inference_time = datetime.timedelta(seconds=current_time - prev_time)\n","    prev_time = current_time\n","    print(\"\\t+ Batch %d, Inference Time: %s\" % (batch_i, inference_time))\n","\n","    # Save image and detections\n","    imgs.extend(img_paths)\n","    img_detections.extend(detections)\n","\n","# Bounding-box colors\n","cmap = plt.get_cmap(\"tab20b\")\n","colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n","\n","print(\"\\nSaving images:\")\n","# Iterate through images and save plot of detections\n","for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n","\n","    print(\"(%d) Image: '%s'\" % (img_i, path))\n","\n","    # Create plot\n","    img = np.array(Image.open(path))\n","    plt.figure()\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(img)\n","\n","    # Draw bounding boxes and labels of detections\n","    if detections is not None:\n","        # Rescale boxes to original image\n","        detections = rescale_boxes(detections, IMG_SIZE, img.shape[:2])\n","        unique_labels = detections[:, -1].cpu().unique()\n","        n_cls_preds = len(unique_labels)\n","        bbox_colors = random.sample(colors, n_cls_preds)\n","        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n","\n","            print(\"\\t+ Label: %s, Conf: %.5f\" % (classes[int(cls_pred)], cls_conf.item()))\n","\n","            box_w = x2 - x1\n","            box_h = y2 - y1\n","\n","            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n","            # Create a Rectangle patch\n","            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n","            # Add the bbox to the plot\n","            ax.add_patch(bbox)\n","            # Add label\n","            plt.text(\n","                x1,\n","                y1,\n","                s=classes[int(cls_pred)],\n","                color=\"white\",\n","                verticalalignment=\"top\",\n","                bbox={\"color\": color, \"pad\": 0},\n","            )\n","\n","    # Save generated image with detections\n","    plt.axis(\"off\")\n","    plt.gca().xaxis.set_major_locator(NullLocator())\n","    plt.gca().yaxis.set_major_locator(NullLocator())\n","    filename = path.split(\"/\")[-1].split(\".\")[0]\n","    plt.savefig(f\"output/{filename}.png\", bbox_inches=\"tight\", pad_inches=0.0)\n","    plt.show()\n","    plt.close()"],"execution_count":null,"outputs":[]}]}