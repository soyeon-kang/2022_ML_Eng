{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YbQFETSTNuJ-"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","source":["# Deep Learning: Neural Networks\n","Essentially a branch of AI that closely tries to mimic how the human brain works. \n","- Just like humans learn from experience, a deep learning algorithm can perform a task repeatedly, each time tweaking it to improve the outcome.\n","\n","- In traditional machine learning, the algorithm is given a set of relevant features to analyze. However, in deep learning, the algorithm is given raw data and decides for itself what features are relevant.\n","\n","*Training these Neural Networks take a lot of computational power. Pytorch really helps in the computational aspect as well as simplicity in programming the processes!*\n","\n","https://www.youtube.com/watch?v=bfmFfD2RIcg (0:43)\n","![picture](https://drive.google.com/uc?id=1oL0IRADhSzDcwGRxOMh4T_JpetT3oEt1)"],"metadata":{"id":"WKoMUwqlB__1"}},{"cell_type":"markdown","source":["## 4 Steps to training Deep Learning Models\n","\n","1. Data\n","2. **Model**\n","3. objective function; loss function\n","4. optimizer\n","\n","We have already seen how to process the data in previous session!\n","This section will focus on the implementation of the Model."],"metadata":{"id":"oVif8-T9PCdI"}},{"cell_type":"markdown","metadata":{"id":"r_ZtgoubNuJ_"},"source":["\n","# Build the Neural Network\n","\n","Neural networks comprise of layers/modules that perform operations on data.\n","\n","The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to\n","build your own neural network. \n","\n","Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n","A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n","building and managing complex architectures easily.\n","\n","In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ew6RCxF6NuKA","executionInfo":{"status":"ok","timestamp":1648523761540,"user_tz":-540,"elapsed":7028,"user":{"displayName":"CDJ","userId":"01646918403990549302"}}},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{"id":"qFYAMEtPNuKA"},"source":["## Get Device for Training\n","We want to be able to train our model on a hardware accelerator like the GPU,\n","if it is available. \n","\n","Let's check to see if\n","[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we continue to use the CPU.\n","\n","recall: If youâ€™re using Colab, allocate a GPU by going to `Runtime > Change runtime type > GPU`.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2sf4nm4kNuKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514359521,"user_tz":-540,"elapsed":760,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"13e5af2c-c1ac-4841-f698-3f9c5a50184c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"jkEAMx2BNuKA"},"source":["## Define the Class\n","We define our neural network by subclassing ``nn.Module``.\n","\n","- Initialize the neural network layers in ``__init__``. \n","\n","- Every ``nn.Module`` subclass implements\n","the operations on input data in the ``forward`` method. \n","\n","- Using a class to define our neural network makes the code very clean, and easily alterable.\n","\n","We will break this down further below!\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Snk6ZwnVNuKA","executionInfo":{"status":"ok","timestamp":1648514352064,"user_tz":-540,"elapsed":1,"user":{"displayName":"CDJ","userId":"01646918403990549302"}}},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"qegvlQkDNuKA"},"source":["We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print its structure.\n","\n","By moving it to the GPU device, it allows to perform faster computations due to the GPU's ability to compute matrix operations quickly.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Oj_pmqnJNuKB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514369730,"user_tz":-540,"elapsed":8377,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"84bc3cd3-f5ab-4f79-aa2c-b3aef3a362b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["model = NeuralNetwork().to(device)\n","print(model) # For sanity! always check to see if this is what your structure intended!"]},{"cell_type":"markdown","metadata":{"id":"IQAPCj28NuKB"},"source":["To use the model, we pass it the input data. This executes the model's ``forward``,\n","along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n","- Do not call ``model.forward()`` directly!\n","\n","Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class.\n","We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n","\n","Softmax or also known as the \"squashing\" function makes any range of values sum to 1, or in other words converts it into a probability distribution!\n","- The network does not output a probability but rather scalar values called logits.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"J8ARRbJ3NuKB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514729445,"user_tz":-540,"elapsed":4,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"7b72376b-7847-4c9c-9dd5-e885f1adc845"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1024, 0.0992, 0.1028, 0.0882, 0.0903, 0.1076, 0.1050, 0.1076, 0.0939,\n","         0.1030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","Predicted class: tensor([7], device='cuda:0')\n"]}],"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X) # an effect of inheriting nn.Module\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(pred_probab)\n","print(f\"Predicted class: {y_pred}\")"]},{"cell_type":"markdown","metadata":{"id":"yCzcBQzENuKB"},"source":["Model Layers\n","-------------------------\n","\n","Let's break down the layers in the FashionMNIST model. To illustrate it, we\n","will take a sample minibatch of 3 images of size 28x28 and see what happens to it as\n","we pass it through the network.\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"UDUB87LzNuKB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514820195,"user_tz":-540,"elapsed":626,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"1edecd0a-d98e-47ae-c194-209f35a6c7bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}],"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"]},{"cell_type":"markdown","metadata":{"id":"7KDeEhIcNuKC"},"source":["### nn.Flatten\n","\n","We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n","layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\n","the minibatch dimension (at dim=0) is maintained).\n","\n","**Why do we flatten?**\n","\n","Because we are using a Fully connected neural network layers! If we use a convolutional neural network, the height and width of the network should be maintained.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NHsbgF_hNuKC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514828840,"user_tz":-540,"elapsed":485,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"390f6033-6371-4339-d490-649c7ed1ee3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}],"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"]},{"cell_type":"markdown","metadata":{"id":"43YCIy2iNuKC"},"source":["### nn.Linear\n","\n","The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","is a module that applies a linear transformation on the input using its stored weights and biases.\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"464acK2CNuKC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514879979,"user_tz":-540,"elapsed":582,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"640d025d-01ab-4161-a9a7-5c2cc4e4da65"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}],"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"]},{"cell_type":"markdown","metadata":{"id":"pefblkqTNuKC"},"source":["### nn.ReLU\n","\n","Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n","They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n","learn a wide variety of phenomena.\n","\n","In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our\n","linear layers, but there's other activations to introduce non-linearity in your model.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"cffYKVK1NuKC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648514962936,"user_tz":-540,"elapsed":445,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"575dcd5c-3f84-4181-aead-8eaf641d6861"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[ 0.2975, -0.4780, -0.5673,  0.1773, -0.0771,  0.0989,  0.1470, -0.1311,\n","          0.4330, -0.4072,  0.1351, -0.1589, -0.7450, -0.1268, -0.6485,  0.2235,\n","         -0.6105,  0.2965,  0.0558, -0.4895],\n","        [ 0.5204, -0.4248, -0.5295,  0.4683, -0.1476,  0.2489,  0.4604, -0.3918,\n","          0.1053, -0.7214, -0.1279, -0.0559, -0.5307,  0.3835, -0.4957,  0.0658,\n","         -0.3237, -0.1884,  0.2070, -0.3767],\n","        [ 0.4987, -0.0744, -0.2513,  0.2511, -0.2907, -0.0714,  0.5542, -0.3855,\n","          0.3734, -0.6238,  0.0904, -0.5368, -0.6176, -0.1461, -0.1862, -0.2268,\n","         -0.1915,  0.0299, -0.1312, -0.1932]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU: tensor([[0.2975, 0.0000, 0.0000, 0.1773, 0.0000, 0.0989, 0.1470, 0.0000, 0.4330,\n","         0.0000, 0.1351, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.0000, 0.2965,\n","         0.0558, 0.0000],\n","        [0.5204, 0.0000, 0.0000, 0.4683, 0.0000, 0.2489, 0.4604, 0.0000, 0.1053,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.3835, 0.0000, 0.0658, 0.0000, 0.0000,\n","         0.2070, 0.0000],\n","        [0.4987, 0.0000, 0.0000, 0.2511, 0.0000, 0.0000, 0.5542, 0.0000, 0.3734,\n","         0.0000, 0.0904, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0299,\n","         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"]}],"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"]},{"cell_type":"markdown","metadata":{"id":"BBs9vhPBNuKC"},"source":["### nn.Sequential\n","\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered\n","container of modules. The data is passed through all the modules in the same order as defined. You can use\n","sequential containers to put together a quick network like ``seq_modules``.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pQH43DJlNuKD","executionInfo":{"status":"ok","timestamp":1648515001239,"user_tz":-540,"elapsed":576,"user":{"displayName":"CDJ","userId":"01646918403990549302"}}},"outputs":[],"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"]},{"cell_type":"markdown","metadata":{"id":"pK5rodeZNuKD"},"source":["###  nn.Softmax\n","\n","The last linear layer of the neural network returns `logits` - raw values in $[-\\infty, \\infty]$ - which are passed to the\n","[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values\n","[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n","which the values must sum to 1.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Xpb18ZOANuKD","executionInfo":{"status":"ok","timestamp":1648515101783,"user_tz":-540,"elapsed":464,"user":{"displayName":"CDJ","userId":"01646918403990549302"}}},"outputs":[],"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"]},{"cell_type":"markdown","metadata":{"id":"__wfFkAVNuKD"},"source":["Model Parameters\n","-------------------------\n","Many layers inside a neural network are *parameterized*, i.e. have associated weights\n","and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n","tracks all fields defined inside your model object, and makes all parameters\n","accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n","\n","In this example, we iterate over each parameter, and print its size and a preview of its values.\n","- Can be useful for debugging\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"PYjGibNUNuKD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648515139849,"user_tz":-540,"elapsed":441,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"63b575ac-88cd-419d-db5d-4ac2327648e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 3.4440e-02, -4.7709e-05, -3.4345e-03,  ..., -1.9932e-02,\n","         -2.4304e-02, -3.3485e-02],\n","        [-2.4204e-02,  7.9442e-03, -6.5458e-04,  ...,  6.5797e-03,\n","         -3.5401e-02,  6.4008e-03]], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0225, -0.0005], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0276,  0.0367,  0.0152,  ...,  0.0101,  0.0441, -0.0352],\n","        [ 0.0335,  0.0244, -0.0211,  ..., -0.0395,  0.0305, -0.0221]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0432, 0.0077], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0352,  0.0210, -0.0168,  ...,  0.0229,  0.0009,  0.0411],\n","        [ 0.0233,  0.0154,  0.0057,  ..., -0.0429,  0.0289,  0.0192]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0015, -0.0274], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n"]}],"source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"cell_type":"markdown","source":["## Exercise\n","\n","Let's try to create our own Neural Network!\n","- input will be $16 \\times 16$,\n","- 2 layers with \n","- 256 hidden units in first layer,\n","- 10 units in the output layer.\n","\n"],"metadata":{"id":"iNFWjPHSlELl"}},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        ###############Let's fill in below#################\n","        # Hint: nn.Sequential and 2 nn.Linear, 1 ReLU()\n","        \n","\n","        ###################################################\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n"],"metadata":{"id":"KSPL5H4KK1PU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_image = torch.rand(10,16,16)\n","\n","####### FILL IN BELOW ############\n","# Initialize the network!\n","\n","# Print the network to see if this is what you wanted!\n","\n","# inference the input_image through the network we created and defined above!!\n","\n","# print the shape of the output also!!\n","\n"],"metadata":{"id":"Ffx0DCjxmnvx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        ###############Let's fill in below#################\n","        # Hint: nn.Sequential and 2 nn.Linear, 1 ReLU()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(16*16, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 10),\n","        )\n","        ###################################################\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n"],"metadata":{"id":"CJR6Z9ZZlfxG","executionInfo":{"status":"ok","timestamp":1648515285848,"user_tz":-540,"elapsed":436,"user":{"displayName":"CDJ","userId":"01646918403990549302"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["input_image = torch.rand(10,16,16)\n","\n","# Initialize the network!\n","model = NeuralNetwork()\n","# Print the network to see if this is what you wanted!\n","print(model)\n","\n","# TODO inference the input_image through the network we created and defined above!!\n","out = model(input_image)\n","# print the shape of the output also!!\n","\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Muxdn7UimZ59","executionInfo":{"status":"ok","timestamp":1648455446303,"user_tz":-540,"elapsed":3,"user":{"displayName":"CDJ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhNCRY_P6sYQaPX9YUmgkXmQwldQsBwHlEzRWlKg=s64","userId":"01646918403990549302"}},"outputId":"573d091e-ebef-4de9-e144-363956ee9ff9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"TElugDLMNuKD"},"source":["## Further Reading\n","\n","- [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"4. buildmodel_tutorial_sol.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}