{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wB8dB2PxJJ7-"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"wGKnwaPmJJ8C"},"source":["\n","\n","Tensors\n","==========================\n","\n","Tensors are a specialized data structure that are very similar to arrays and matrices.\n","In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n","\n","Tensors are similar to `NumPy’s ndarrays, except that **tensors can run on GPUs or other hardware accelerators.** \n","\n","- tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see `bridge-to-np-label`). \n","- Tensors\n","are also optimized for automatic differentiation. Which is critical for deep learning!\n","\n","![picture](https://drive.google.com/uc?id=1inz8rB_q_mGuSsYz5dRl4C9-9qLTg5Xm)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9H3zGAZCJJ8D","executionInfo":{"status":"ok","timestamp":1648599747543,"user_tz":-540,"elapsed":5937,"user":{"displayName":"CDJ","userId":"01646918403990549302"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"we97KHhwJJ8E"},"source":["## Initializing a Tensor\n","\n","Tensors can be initialized in various ways. Take a look at the following examples:\n","\n","**Directly from data**\n","\n","Tensors can be created directly from data. The data type is automatically inferred.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VgLJ-QKYJJ8E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599747543,"user_tz":-540,"elapsed":4,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"dbca2512-0eed-4ff7-e985-4e576cf28d26"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)\n","print(x_data)"]},{"cell_type":"markdown","metadata":{"id":"BFVJ7liTJJ8E"},"source":["**From a NumPy array**\n","\n","Tensors can be created from NumPy arrays (and vice versa).\n","\n","This becomes handy when you would want to pre-process on NumPy using many of Numpy functions and convert to Tensor for GPU advantageous functions.\n","Same goes for post-processing! (converting Tensor to NumPy)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"q6RyZM4bJJ8F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599747543,"user_tz":-540,"elapsed":3,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"c123f6ae-e043-434c-b0e5-54447b99026b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)\n","print(x_np)"]},{"cell_type":"markdown","metadata":{"id":"Smfse93kJJ8G"},"source":["**From another tensor creating operation:**\n","\n","The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"AYAUm8DkJJ8G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599748285,"user_tz":-540,"elapsed":2,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"b11e2845-0ebb-46c8-b855-41d38f206849"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.8346, 0.6601],\n","        [0.7714, 0.1391]]) \n","\n","Zeros Tensor: \n"," tensor([[0, 0],\n","        [0, 0]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")\n","\n","x_zeros = torch.zeros_like(x_data)\n","print(f\"Zeros Tensor: \\n {x_zeros} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"APk23BUAJJ8H"},"source":["**With random or constant values:**\n","\n","``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"kDN97kT7JJ8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599749301,"user_tz":-540,"elapsed":1,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"d31f3453-9f67-4b5f-c82f-b1607f7781b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.8970, 0.2701, 0.2081],\n","        [0.9302, 0.1464, 0.6771]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"markdown","metadata":{"id":"_byws98pJJ8I"},"source":["## Attributes of a Tensor\n","\n","Tensor attributes describe their **shape, datatype, and the device** on which they are stored.\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wXnnC-2vJJ8I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599751046,"user_tz":-540,"elapsed":4,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"67d45dcb-048e-4e44-a5fb-bdbf3dd59978"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"aE2ejTGDJJ8J"},"source":["## Operations on Tensors\n","\n","Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,\n","indexing, slicing), sampling and more are\n","comprehensively described [here](https://pytorch.org/docs/stable/torch.html).\n","\n","Each of these operations can be run on the GPU (at typically higher speeds than on a CPU!). \n","\n","If you’re using Colab, allocate a GPU by going to `Runtime > Change runtime type > GPU`.\n","\n","By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using\n","``.to`` method (after checking for GPU availability). Keep in mind that copying large tensors\n","across devices can be expensive in terms of time and memory!\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gPQV9MxsJJ8J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599807904,"user_tz":-540,"elapsed":238,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"4682b030-8aec-47eb-c494-29a11cfbb29f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device tensor is stored on: cuda:0\n"]}],"source":["tensor = torch.rand(3,4) # re-init random tensor\n","# We move our tensor to the GPU if available\n","if torch.cuda.is_available():\n","    tensor = tensor.to(\"cuda\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"vdgPa2r6JJ8J"},"source":["Try out some of the operations from the list.\n","If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dhCr1XvyJJ8J"},"source":["**Standard numpy-like indexing and slicing:**\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"D1gEywihJJ8K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648599980828,"user_tz":-540,"elapsed":441,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"3b8d6b53-c355-40f1-c54e-3960a4eba144"},"outputs":[{"output_type":"stream","name":"stdout","text":["First row: tensor([1., 1., 1., 1.])\n","First column: tensor([1., 1., 1.])\n","Last column: tensor([1., 1., 1.])\n","Last column: tensor([1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(3, 4)\n","print(f\"First row: {tensor[0]}\")\n","print(f\"First column: {tensor[:, 0]}\")\n","print(f\"Last column: {tensor[..., -1]}\")\n","print(f\"Last column: {tensor[:, -1]}\")\n","\n","# remember [row x column]. So select all rows in selected 1 column and set it to 0.\n","tensor[:,1] = 0 \n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"4-g1IJ-lJJ8K"},"source":["**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n","See also [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)\n","another tensor joining op that is subtly different from ``torch.cat``.\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"pWkMmAt7JJ8K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648600401861,"user_tz":-540,"elapsed":317,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"3deac069-d63c-45a7-aac9-bd81cee5d2b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 4])\n","torch.Size([4, 3])\n","tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n","torch.Size([3, 12])\n"]}],"source":["print(tensor.shape)\n","print(tensor.T.shape)\n","t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)\n","print(t1.shape)"]},{"cell_type":"markdown","metadata":{"id":"0QfZ2ZCAJJ8K"},"source":["**Arithmetic operations**\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"CWxWN9pVJJ8K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648600629585,"user_tz":-540,"elapsed":3,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"d6eafa45-f681-4ff9-adb4-64fa7dc118f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3., 3., 3.],\n","        [3., 3., 3.],\n","        [3., 3., 3.]])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: An output with one or more elements was resized since it had shape [3, 4], which does not match the required output shape [3, 3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  \n"]}],"source":["# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","\n","y3 = torch.rand_like(tensor)\n","torch.matmul(tensor, tensor.T, out=y3)\n","\n","print(y1)\n","\n","# This computes the element-wise product. z1, z2, z3 will have the same value\n","z1 = tensor * tensor\n","z2 = tensor.mul(tensor)\n","\n","z3 = torch.rand_like(tensor)\n","torch.mul(tensor, tensor, out=z3)\n","\n","print(z1)"]},{"cell_type":"markdown","metadata":{"id":"L_HTgPBHJJ8K"},"source":["**Single-element tensors** If you have a one-element tensor, for example by aggregating all\n","values of a tensor into one value, you can convert it to a Python\n","numerical value using ``item()``:\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"3cbJAkJfJJ8L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648600737009,"user_tz":-540,"elapsed":4,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"55c67d37-6beb-4c79-912f-d14f7f890fc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(9.)\n","9.0 <class 'float'>\n"]}],"source":["agg = tensor.sum()\n","agg_item = agg.item()\n","print(agg)\n","print(agg_item, type(agg_item))"]},{"cell_type":"markdown","metadata":{"id":"lHLZIEi4JJ8L"},"source":["**In-place operations**\n","Operations that store the result into the operand are called in-place. \n","\n","Normally, python will copy and create new memory for new variables, using an in-place operation, no new memory will be used.\n","They are denoted by a ``_`` suffix.\n","\n","For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"f64CWKBGJJ8L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648600961319,"user_tz":-540,"elapsed":308,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"7dada41b-932c-43cb-dc16-a43dd3e42346"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]]) \n","\n","tensor([[6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.]])\n"]}],"source":["print(f\"{tensor} \\n\")\n","tensor.add_(5)\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"1KDjBwQQJJ8L"},"source":["**Note** In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ciXchZhnJJ8L"},"source":["\n","## Bridge with NumPy\n","Tensors on the CPU and NumPy arrays can *__share__ their underlying memory\n","locations*, and changing one will change\tthe other.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V6PwlhYAJJ8L"},"source":["Tensor to NumPy array\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"UNx2CEQuJJ8L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648601044412,"user_tz":-540,"elapsed":236,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"ed39e831-58e5-4865-e5d9-75896eb82089"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"]}],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"Cu655EsoJJ8L"},"source":["A change in the tensor is also reflected in the NumPy array. (because they share the memory)\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"VYR_uYf0JJ8L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648601069735,"user_tz":-540,"elapsed":237,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"c0889b29-d174-4654-a02e-f1fd160e0ba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.])\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"gMhGSyc8JJ8L"},"source":["NumPy array to Tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtBf_j32JJ8M"},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"]},{"cell_type":"markdown","metadata":{"id":"0YLswjKjJJ8M"},"source":["Changes in the NumPy array reflects in the tensor.\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"2jmgYXdgJJ8M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648601122141,"user_tz":-540,"elapsed":342,"user":{"displayName":"CDJ","userId":"01646918403990549302"}},"outputId":"c54c80ee-921c-4f66-f6c2-14eae8808c84"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([3., 3., 3., 3., 3.])\n","n: [3. 3. 3. 3. 3.]\n"]}],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","source":["## Quiz\n"],"metadata":{"id":"4M-Eohy5V9HM"}},{"cell_type":"markdown","source":["1. Create a random tensor called `rand_tensor`, tensor of all 1s called `one_tensor`, tensor of all 0s called `zero_tensor` ALL of size `3 x 4`, and print the tensors, and print its attributes (recall: device, datatype, shape). If the tensor is on CPU, place it on GPU!"],"metadata":{"id":"DMJe_fgaWKeh"}},{"cell_type":"code","source":[""],"metadata":{"id":"AvFgOdJfV_mS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. create a numpy array of all 1s of size `3 x 4`. Convert them to pytorch Tensors, print the tensors & its attributes. Again if the tensor is on CPU, place it on GPU!"],"metadata":{"id":"gxrOGyOLX_0M"}},{"cell_type":"code","source":[""],"metadata":{"id":"EUWdbSDeV_zC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. We are going to do matrix multiplication. Create a Tensor that is able to multiply to our `one_tensor` created above, and perform the matrix multiplication."],"metadata":{"id":"U-oHZe-TZDDV"}},{"cell_type":"code","source":[""],"metadata":{"id":"O_sxOt03V_9A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Create a Tensor to \"concatenate\" to the row (first dimension) of our `zero_tensor` above. Create another Tensor to \"stack\" on `dim=0` to our `zero_tensor` as well. Print the shapes of all created tensors. "],"metadata":{"id":"fbS7iiq3ZiqC"}},{"cell_type":"code","source":[""],"metadata":{"id":"iwEYJ5obWAJK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Create a tensor of size (3,4) and [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) it to (6,2). Print the resulting shape of the tensor."],"metadata":{"id":"1iuq9jpeaIjt"}},{"cell_type":"code","source":[""],"metadata":{"id":"N1Dd8vzsWB_O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Create a linearly interpolation of $x$ values where $0 <= x <= 10$ evenly spaced by 2. (Hint: [linspace](https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace))"],"metadata":{"id":"o7vN7ydmbfBl"}},{"cell_type":"code","source":[""],"metadata":{"id":"8rZKrYXNWCF4"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"1. tensors.ipynb","provenance":[{"file_id":"1-4reEb0LOg_LOd-I9oGyxLbZ4LJVP0ld","timestamp":1648522572104}],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}