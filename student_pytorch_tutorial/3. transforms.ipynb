{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"H5r3KlQpNt0O"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"rcXM4EomNt0P"},"source":["\n","Transforms\n","===================\n","\n","Data does not always come in its final processed form that is required for\n","training machine learning algorithms. We use **transforms** to perform some\n","manipulation of the data and make it suitable for training.\n","\n","All TorchVision datasets have two parameters -``transform`` to modify the features and\n","``target_transform`` to modify the labels - that accept callables containing the transformation logic.\n","The [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html) module offers\n","several commonly-used transforms out of the box.\n","\n","The FashionMNIST features are in PIL Image format, and the labels are integers.\n","For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\n","To make these transformations, we use ``ToTensor`` and ``Lambda``.\n","\n","Broken down and explained below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfJBzmSLNt0Q"},"outputs":[],"source":["import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda\n","\n","ds = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(), # can also take multiple transforms using `Compose`!\n","    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",")"]},{"cell_type":"markdown","metadata":{"id":"undpakdSNt0R"},"source":["## ToTensor()\n","\n","[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n","converts a PIL image or NumPy ``ndarray`` into a ``FloatTensor``. and scales\n","the image's pixel intensity values in the range [0., 1.]\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JRasHEpcNt0R"},"source":["## Lambda Transforms (Custom Transforms!)\n","\n","Lambda transforms apply any user-defined lambda function. Here, we define a function\n","to turn the integer into a one-hot encoded tensor.\n","It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls\n","[scatter](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html) which *assigns a ``value=1`` on the index as given by the label ``y``.*\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHGwmQXnNt0R"},"outputs":[],"source":["target_transform = Lambda(lambda y: torch.zeros(\n","    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"]},{"cell_type":"markdown","metadata":{"id":"267CdjaPNt0R"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","source":["## Exercise\n","Let's try creating a custom transformation for the Kuzushiji-MNIST data abbreviated as `KMNIST`. [KMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.KMNIST.html?highlight=kmnist#torchvision.datasets.KMNIST)\n","\n","You will be applying not one, but **2** transformations using `Compose`:\n","1. [CenterCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html?highlight=centercrop#torchvision.transforms.CenterCrop) with a size of 20\n","2. `ToTensor()`\n","\n","Also one-hot encoding the target integer value will also be done.\n","\n","Hint: The [Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose) function will come in handy! Let's get used to reading the documentations!\n"],"metadata":{"id":"nAEE15xw4MEd"}},{"cell_type":"code","source":["from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda, CenterCrop, Compose\n","\n","##### TODO #######\n","# transform_sequential = [] # fill in this list!\n","# KMNIST_transforms = # Hint: use `Compose`.\n","# target_onehot = # Hint: hmm.. We have seen it just now!\n","\n","# Hint: follow above, but its a different dataset!\n","# ds = "],"metadata":{"id":"GN-RXL2kUryF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rg4OX8ZjNt0S"},"source":["### Further Reading\n","- [torchvision.transforms API](https://pytorch.org/vision/stable/transforms.html)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"3. transforms.ipynb","provenance":[{"file_id":"1HasD2eoJm53YAIqSBw1M9R-KdlyMzvCC","timestamp":1648523594697}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}