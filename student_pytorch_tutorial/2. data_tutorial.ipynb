{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AtOOj88-KO5l"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"bf9jflPfKO5m"},"source":["\n","\n","# Datasets & DataLoaders"]},{"cell_type":"markdown","metadata":{"id":"bbRve6rdKO5n"},"source":["Code for processing data samples can get messy and hard to maintain.\n","\n","We ideally want our dataset code\n","to be *decoupled* from our model training code for better readability and modularity.\n","\n","PyTorch provides two data primitives: ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``\n","that allow you to use pre-loaded datasets as well as your own data.\n","``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n","the ``Dataset`` to enable easy access to the samples.\n","\n","PyTorch domain libraries provide a number of pre-loaded datasets (such as MNIST, FashionMNIST, SVHN, CIFAR10 etc) that\n","subclass ``torch.utils.data.Dataset`` and implement functions specific to the particular data.\n","\n","They can be used to prototype and benchmark your model. You can find them\n","here: [Image Datasets](https://pytorch.org/vision/stable/datasets.html),\n","[Text Datasets](https://pytorch.org/text/stable/datasets.html), and\n","[Audio Datasets](https://pytorch.org/audio/stable/datasets.html).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zNM4LJJwKO5o"},"source":["Loading a Dataset\n","-------------------\n","\n","Here is an example of how to load the [Fashion-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/) dataset from TorchVision.\n","Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples.\n","\n","Each example comprises a $28×28$ grayscale image and an associated label from one of 10 classes.\n","\n","We load the [FashionMNIST Dataset](https://pytorch.org/vision/stable/datasets.html#fashion-mnist) with the following parameters:\n"," - ``root`` is the path where the train/test data is stored,\n"," - ``train`` specifies training or test dataset,\n"," - ``download=True`` downloads the data from the internet if it's not available at ``root``.\n"," - ``transform`` and ``target_transform`` specify the feature and label transformations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEHBGbWVKO5o"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")"]},{"cell_type":"markdown","metadata":{"id":"QzsrzswuKO5o"},"source":["Iterating and Visualizing the Dataset\n","-----------------\n","\n","We can index ``Datasets`` manually like a list: ``training_data[index]``.\n","Lets use ``matplotlib`` to visualize some samples in our training data.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAkvUjuKKO5p"},"outputs":[],"source":["labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    img, label = training_data[sample_idx]\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JeJkH3uzKO5q"},"source":["Creating a Custom Dataset for your files\n","---------------------------------------------------\n","\n","A custom Dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`.\n","Take a look at this implementation; the FashionMNIST images are stored\n","in a directory ``img_dir``, and their labels are stored separately in a CSV file ``annotations_file``.\n","\n","In the next sections, we'll break down what's happening in each of these functions.\n","\n","*NOTE: this will be necessary when you go back to work and want to employ machine learning to your OWN DATASET!*\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPcVFv88KO5q"},"outputs":[],"source":["import os\n","import pandas as pd\n","from torchvision.io import read_image\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"UT4O6rUyKO5r"},"source":["### \\_\\_init\\_\\_\n","The `__init__` function is run once when instantiating the Dataset object. \n","We initialize\n","- the directory containing the images\n","- the annotations file,\n","- transforms for both input and target \n","\n","Note that you have the freedom to initialize whatever you like, but below is some minimal and useful initializations.\n","\n","The labels.csv file looks like:\n","\n","    tshirt1.jpg, 0\n","    tshirt2.jpg, 0\n","    ......\n","    ankleboot999.jpg, 9\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BylNKVCsKO5r"},"outputs":[],"source":["def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","    self.img_labels = pd.read_csv(annotations_file)\n","    self.img_dir = img_dir\n","    self.transform = transform\n","    self.target_transform = target_transform"]},{"cell_type":"markdown","metadata":{"id":"zs7Bs8J7KO5r"},"source":["### \\_\\_len__\n","\n","The __len__ function returns the number of samples in our dataset.\n","\n","Example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZYsb0l4KO5r"},"outputs":[],"source":["def __len__(self):\n","    return len(self.img_labels)"]},{"cell_type":"markdown","metadata":{"id":"EagX4FlQKO5r"},"source":["### \\_\\_getitem__\n","\n","\n","The __getitem__ function loads and returns a sample from the dataset at the given index ``idx``.\n","Based on the index, \n","1. it identifies the image's location on disk\n","2. converts that to a tensor using ``read_image``\n","3. retrieves the corresponding label from the csv data in ``self.img_labels``.\n","4. calls the transform functions on them (if applicable). \n","5. returns the tensor image and corresponding label in a tuple."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zyhu94ZKO5s"},"outputs":[],"source":["def __getitem__(self, idx):\n","    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","    image = read_image(img_path)\n","    label = self.img_labels.iloc[idx, 1]\n","    if self.transform:\n","        image = self.transform(image)\n","    if self.target_transform:\n","        label = self.target_transform(label)\n","    return image, label"]},{"cell_type":"markdown","source":["### Why Transform?\n","Helps when you have...\n","- very little data\n","- data containing points that are not diverse\n","- for self-supervised learning (will learn later)\n","- regularization to prevent overfitting\n","\n","### Some common existing transforms include\n","- shift\n","- scale\n","- Gaussian noise\n","- rotate\n","- flip (vertical or horizontal)\n","- mixup\n","- color augment\n","- etc..\n"],"metadata":{"id":"8_wuzq7-1Z1n"}},{"cell_type":"markdown","metadata":{"id":"fQ2U7aakKO5s"},"source":["## Preparing your data for training with DataLoaders\n","\n","The ``Dataset`` retrieves our dataset's features and labels one sample at a time. \n","\n","While training a model, we typically want to\n","pass samples in **\"minibatches\", reshuffle** the data at every epoch to reduce model overfitting, and use Python's ``multiprocessing`` to\n","speed up the data retrieval.\n","\n","``DataLoader`` is an iterable that abstracts this complexity for us in an easy API.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSce6jVgKO5s"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"5Qnz3Eg_KO5s"},"source":["Iterate through the DataLoader\n","--------------------------\n","\n","We have loaded that dataset into the ``DataLoader`` and can iterate through the dataset as needed.\n","Each iteration below returns a batch of ``train_features`` and ``train_labels`` (containing ``batch_size=64`` features and labels respectively).\n","Because we specified ``shuffle=True``, after we iterate over all batches the data is shuffled (for finer-grained control over\n","the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler>).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzE4yt3nKO5s"},"outputs":[],"source":["# Display image and label.\n","train_features, train_labels = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0].squeeze()\n","label = train_labels[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"]},{"cell_type":"markdown","source":["## Exercise 1\n","Let's try importing a pre-built dataset, [KMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.KMNIST.html?highlight=kmnist#torchvision.datasets.KMNIST).\n","Kuzushiji-MNIST, a cursive writing style in Japan for hundreds of years.\n","- load into training and test dataset.\n","- load onto a Dataloader with batch size 32 and no shuffling for testing.\n","- display the images using the created iterator."],"metadata":{"id":"dJzfoUr33Djs"}},{"cell_type":"code","source":["# load into training and test dataset.\n","\n","\n","# Set-up the Dataloader.\n","\n","\n","\n","# display the images using the created iterator."],"metadata":{"id":"pMizGDIsYpmb"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"2. data_tutorial.ipynb","provenance":[{"file_id":"1DPGuQGBSWGvPngOjiEo_j3paCh2XTW1s","timestamp":1648523187138},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/57a471142057f27da635118e88a99bf6/data_tutorial.ipynb","timestamp":1648431286058}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}